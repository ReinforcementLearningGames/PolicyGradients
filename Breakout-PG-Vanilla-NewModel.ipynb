{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout - Policy Gradient Method\n",
    "## Policy Gradients Vanilla type\n",
    "* Borrowed heavily from structure of: [Vanilla Policy Gradient](https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb)\n",
    "* In order to determine positive and negative rewards we must associate it with some direction of the pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, convolutional, pooling, Flatten, Dropout, Input, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as Bk\n",
    "from scipy.misc import imread, imshow\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-18 20:41:14,721] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "Discrete(6)\n",
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "print(env.observation_space)  # 210v x 160w x 3color Box (sparse tensor!!!)\n",
    "print(env.action_space)       # 6x1 Discrete\n",
    "s = env.reset()\n",
    "print(s.shape)\n",
    "gamma = 0.99\n",
    "try: \n",
    "    xrange(1)\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rl_agent():\n",
    "    def __init__(self,  \n",
    "                 environment, \n",
    "                 state_space_size, \n",
    "                 action_space_size,               \n",
    "                 hidden_units = 512,\n",
    "                 learning_rate = 1e-4,             # Lambda or other for gradient descent\n",
    "                 epsilon = 1e-8,                  # Error - error for types of gradient descent or random choice\n",
    "                 random_choice_threshold = -1.22,\n",
    "                 optimizer = Adam,\n",
    "                 dropout_rate = 0.6,\n",
    "                 load_file_weights = None,\n",
    "                 verbose = 0,\n",
    "                 frames = 3\n",
    "                ):\n",
    "        self.environment = environment\n",
    "        self.hidden_units = hidden_units # hidden neurons\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.optimizer = optimizer\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.load_file_weights = load_file_weights\n",
    "        self.verbose = verbose\n",
    "        self.frames = frames\n",
    "        self.input_dim = 6400\n",
    "        self.prev_processed_state = None\n",
    "        self.past_differences = []\n",
    "        self.render = False\n",
    "        self.random_choice_threshold = random_choice_threshold\n",
    "        \n",
    "    def define_model(self):\n",
    "        # Keras vars\n",
    "        self.model = Sequential()\n",
    "        # Default is channels last\n",
    "        self.model.add(convolutional.Conv2D(filters=64, kernel_size = (3,3), strides=(2,2), padding='valid', input_shape = (80,80, 1)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        print(self.model.layers[-1].input_shape)\n",
    "        print(self.model.layers[-1].output_shape)\n",
    "        self.model.add(convolutional.Conv2D(filters=32, kernel_size = (3,3), strides=(2, 2), padding='same'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(convolutional.Conv2D(filters=64, kernel_size = (3,3), strides=(2, 2), padding='valid'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(convolutional.Conv2D(filters=64, kernel_size = (3,3), strides=(1, 1), padding='valid'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        print(self.model.layers[-1].input_shape)\n",
    "        print(self.model.layers[-1].output_shape)\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(self.hidden_units))   # Karpathy suggested N=200 in the hidden layer\n",
    "        self.model.add(Dropout(self.dropout_rate))      # But since we are doing dropout to avoid overfitting... increase\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(self.hidden_units))   # Karpathy suggested N=200 in the hidden layer\n",
    "        self.model.add(Dropout(self.dropout_rate))      # But since we are doing dropout to avoid overfitting... increase\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        print(self.model.layers[-1].input_shape)\n",
    "        print(self.model.layers[-1].output_shape)\n",
    "        self.model.add(Dense(6, activation='softmax'))  # Actions are a 6x1 vector\n",
    "        optimizer = self.optimizer(lr=self.learning_rate, beta_1=0.9, beta_2=0.999, epsilon = self.epsilon, decay=0.0)\n",
    "        self.model.compile(\n",
    "            optimizer = optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "             self.model.load_weights('saved_weights5.h5')\n",
    "        except:\n",
    "            print(\"Training model without old weights\")\n",
    "            \n",
    "        # State space size will be a 210x160x1 dim\n",
    "        self.state = Input(shape=[80,80,1])\n",
    "        #  self.current_action = tf.argmax(self.output_layer,1)\n",
    "        self.reward_tensor = Input(shape=(1,), dtype='float32')\n",
    "        self.action_tensor = Input(shape=(1,), dtype='int32')\n",
    "        \n",
    "        if self.verbose >= 1:\n",
    "            print(self.model.summary())\n",
    "        \n",
    "    def training(self, max_games = 100000):\n",
    "        mean_rewards_list = []\n",
    "        self.state = self.environment.reset()\n",
    "        chosen_vectors, rewards, rewards_history = [], [], []\n",
    "        running_reward = 0\n",
    "        reward_for_episode = 0\n",
    "        i = 0\n",
    "        while True:\n",
    "            history_element, done = self.choose_action()\n",
    "            chosen_vectors.append(history_element[4]) # dlogps\n",
    "            reward_for_episode += float(history_element[3]) \n",
    "            rewards.append(history_element[3]) # drs\n",
    "            rewards_history.append(history_element[3]) \n",
    "            if done:\n",
    "                i += 1\n",
    "                all_episode_features = np.vstack(agent.past_differences) # epx\n",
    "                plt.imshow(agent.state)\n",
    "                all_episode_chosen_vectors = np.vstack(chosen_vectors) #epdlogp\n",
    "                all_episode_rewards = np.vstack(rewards) # epr\n",
    "                discounted_episode_rewards = apply_gamma(all_episode_rewards)\n",
    "                # Standard normal feature scaling \n",
    "                std_dev = np.std(discounted_episode_rewards)+self.epsilon\n",
    "                discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
    "                discounted_episode_rewards /= std_dev\n",
    "                all_episode_chosen_vectors *= discounted_episode_rewards\n",
    "                self.model.fit(all_episode_features.reshape([-1,80,80,1]), all_episode_chosen_vectors, epochs=1, verbose=0)\n",
    "                self.prev_processed_state = None\n",
    "                reward_for_episode = 0\n",
    "                self.state = self.environment.reset()\n",
    "                self.past_differences = []\n",
    "                chosen_vectors = []\n",
    "                rewards = []\n",
    "                if i % 50 == 0 and i != 0:\n",
    "                    print(len(rewards_history))\n",
    "                    mean_rewards = np.mean(rewards_history[-50:])\n",
    "                    self.saveweights()\n",
    "                    self.savemodel()\n",
    "                    mean_rewards_list.append(mean_rewards)\n",
    "                    print(\"MEAN REWARDS:\" + str(mean_rewards))\n",
    "                    \n",
    "    def savemodel(self):\n",
    "        with open('saved_model5.json', 'w') as modelfile:\n",
    "            modelfile.write(self.model.to_json())\n",
    "            \n",
    "    def saveweights(self):\n",
    "        self.model.save_weights('saved_weights5.h5', overwrite=True)\n",
    "        \n",
    "    def choose_action(self):\n",
    "        #Probabilistically pick an action given our network outputs.\n",
    "        s = self.preprocess(self.state)\n",
    "        self.state = s\n",
    "        self.prev_processed_state = self.prev_processed_state if self.prev_processed_state is not None else np.zeros((1, 80,80, 1), dtype='float32')\n",
    "        difference_processed = (s - self.prev_processed_state)\n",
    "        self.prev_processed_state = s\n",
    "        # difference_processed = difference_processed.flatten()\n",
    "        self.past_differences.append(difference_processed)\n",
    "        self.action_probabilities = self.model.predict(difference_processed.reshape([-1,80,80,1]), batch_size=1).flatten()\n",
    "        e = np.random.normal()\n",
    "        if (e < self.random_choice_threshold):\n",
    "            action = np.random.choice(self.environment.action_space.n, 1)[0]\n",
    "        else:\n",
    "            action = np.random.choice(self.environment.action_space.n, 1,p=self.action_probabilities/np.sum(self.action_probabilities))[0]\n",
    "        self.state, reward, done, info = self.environment.step(action)\n",
    "        if self.render: self.environment.render()\n",
    "        chosen_vector = np.zeros([self.environment.action_space.n])\n",
    "        chosen_vector[action] = 1\n",
    "        return [s,action, self.state, reward, chosen_vector], done\n",
    "    \n",
    "    def preprocess(self, state):\n",
    "        # take a 210x160x3 image and convert to 210x160x1 as 210*160 length vector flattened F = float32 in dim\n",
    "        # From https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html\n",
    "        #r, g, b = state[:, :, 0], state[:, :, 1], state[:, :, 2]\n",
    "        #F = r * 299.0/1000 + g * 587.0/1000 + b * 114.0/1000\n",
    "        # Now using carpathy 0/1 conversion of a majority of the surface area of the screen down to 80x80\n",
    "        F = state\n",
    "        F = F[33:193] # crop vertical rows to 160\n",
    "        F = F[::2,::2,0] # downsample by factor of 2 to 90x80 just keeps the first index of the 3rd dim\n",
    "        F[F != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "        F[F != 1] = 0\n",
    "        return F.reshape([1,80,80,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_gamma(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    r = r.flatten()\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 39, 39, 64)\n",
      "(None, 39, 39, 64)\n",
      "(None, 7, 7, 64)\n",
      "(None, 7, 7, 64)\n",
      "(None, 1024)\n",
      "(None, 1024)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 39, 39, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 39, 39, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 39, 39, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 20, 20, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 20, 20, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 9, 9, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 4,351,654\n",
      "Trainable params: 4,347,110\n",
      "Non-trainable params: 4,544\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "agent = rl_agent(\n",
    "    environment=env,\n",
    "    state_space_size=(6400,1),\n",
    "    action_space_size=6,\n",
    "    hidden_units=1024,\n",
    "    epsilon=1e-8, \n",
    "    optimizer=Adam,\n",
    "    verbose=1)\n",
    "agent.define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27c65cd6f60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZJbkEJklOURmSQ6RWZJDZJb0f4maYI/Zjq5YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27c69d9ecc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s,r,d,i = env.step(0)\n",
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200  72  72]\n"
     ]
    }
   ],
   "source": [
    "print(s[190][115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
