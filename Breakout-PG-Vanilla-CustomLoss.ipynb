{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout - Policy Gradient Method w/ Custom Loss\n",
    "\n",
    "## Policy Gradients Vanilla type\n",
    "* Borrowed heavily from structure of: [Vanilla Policy Gradient](https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb)\n",
    "* In order to determine positive and negative rewards we must associate it with some direction of the pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, convolutional, pooling, Flatten, Dropout, Input, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as Bk\n",
    "from scipy.misc import imread, imshow\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-30 22:34:26,868] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "print(env.observation_space)  # 210v x 160w x 3color Box (sparse tensor!!!)\n",
    "print(env.action_space)       # 6x1 Discrete\n",
    "gamma = 0.99\n",
    "try: \n",
    "    xrange(1)\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rl_agent():\n",
    "    def __init__(self,  \n",
    "                 environment, \n",
    "                 hidden_units = 1024,\n",
    "                 epsilon = 1e-8,                  # Error - error for types of gradient descent or random choice\n",
    "                 learning_rate = .01,\n",
    "                 decay_rate = 1e-7,\n",
    "                 random_choice_threshold = -2,\n",
    "                 optimizer = Adam,\n",
    "                 dropout_rate = 0.5,\n",
    "                 frames = 3,\n",
    "                 input_dim = (80,80,1),\n",
    "                 output_dim = 6,\n",
    "                 weight_iteration = 1\n",
    "                ):\n",
    "        self.environment = environment\n",
    "        self.hidden_units = hidden_units # hidden neurons\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.optimizer = optimizer(lr=learning_rate, decay=decay_rate)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.frames = frames\n",
    "        self.input_dim = input_dim\n",
    "        self.render = False\n",
    "        self.prev_processed_state = None\n",
    "        self.random_choice_threshold = random_choice_threshold\n",
    "        self.output_dim = output_dim\n",
    "        self.weight_i = weight_iteration\n",
    "        self.__define_model(self.output_dim)\n",
    "        self.__build_train_fn() # creates self.train_fn()\n",
    "        \n",
    "    def __define_model(self, output_dim):\n",
    "        # Keras vars\n",
    "        Bk.set_learning_phase(1) #set learning phase\n",
    "        self.model = Sequential()\n",
    "        # Default is channels last\n",
    "        self.model.add(convolutional.Conv2D(filters=32, kernel_size = (2,2), strides=(2,2), padding='same', input_shape = (80,80, 1)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(convolutional.Conv2D(filters=64, kernel_size = (2,2), strides=(2,2), padding='valid'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(convolutional.Conv2D(filters=128, kernel_size = (3,3), strides=(2,2), padding='valid'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(convolutional.Conv2D(filters=256, kernel_size = (3,3), strides=(2,2), padding='valid'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        print(self.model.layers[-1].input_shape)\n",
    "        print(self.model.layers[-1].output_shape)\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(self.hidden_units//2))   # Karpathy suggested N=200 in the hidden layer\n",
    "        self.model.add(Dropout(self.dropout_rate))      # But since we are doing dropout to avoid overfitting... increase\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(self.hidden_units//2))   # Karpathy suggested N=200 in the hidden layer\n",
    "        self.model.add(Dropout(self.dropout_rate))      # But since we are doing dropout to avoid overfitting... increase\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        print(self.model.layers[-1].input_shape)\n",
    "        print(self.model.layers[-1].output_shape)\n",
    "        self.model.add(Dense(self.output_dim, activation='softmax'))  # Actions are a 6x1 vector\n",
    "        \n",
    "        try:\n",
    "             self.model.load_weights('saved_weights-'+str(self.weight_i)+'.h5')\n",
    "        except:\n",
    "            print(\"Training model without old weights\")\n",
    "            \n",
    "        # State space size will be a 210x160x1 dim\n",
    "        self.state = Input(shape=[80,80,1])\n",
    "        \n",
    "        print(self.model.summary())\n",
    "            \n",
    "    def __build_train_fn(self):\n",
    "        # 6x1 tensor\n",
    "        action_prob_placeholder = self.model.output\n",
    "        # 6x1\n",
    "        action_placeholder = Bk.placeholder(shape=(None, self.environment.action_space.n),\n",
    "                                                  name=\"action_vector\")\n",
    "        # nx1\n",
    "        discounted_reward_placeholder = Bk.placeholder(shape=(None,),\n",
    "                                                    name=\"discount_reward\")\n",
    "        # ?? vectorize mult and sum, why not just do action_prob_placeholder transpose * action_onehot??\n",
    "        action_prob = Bk.sum(action_prob_placeholder * action_placeholder, axis=1)\n",
    "        # action_prob = action_prob_placeholder.transpose() * action_placeholder\n",
    "        # log! \n",
    "        log_action_prob = Bk.log(action_prob)\n",
    "\n",
    "        # -loss => because Theta = Theta + alpha * gradient * (gamma * r)\n",
    "        loss = -log_action_prob * discounted_reward_placeholder\n",
    "        loss = Bk.mean(loss)\n",
    "        \n",
    "        # This way we can access the weights\n",
    "        updates = self.optimizer.get_updates(params=self.model.trainable_weights,\n",
    "                                   constraints=[],\n",
    "                                   loss=loss)\n",
    "        \n",
    "        # Feed into the model like the feed_dict in TF\n",
    "        self.train_fn = Bk.function(inputs=[self.model.input,\n",
    "                                           action_placeholder,\n",
    "                                           discounted_reward_placeholder],\n",
    "                                   outputs=[],\n",
    "                                   updates=updates)\n",
    "        \n",
    "    def fit(self, S,A,R):\n",
    "        action_onehot = to_categorical(A, num_classes=self.output_dim)\n",
    "        discounted_normed_reward = apply_gamma(R)\n",
    "        self.train_fn([S, action_onehot, discounted_normed_reward]) \n",
    "        # call what was built in build_train_fn which takes as input:\n",
    "        # a list of [input states, categorical actions, and discounted rewards]\n",
    "    \n",
    "    def start_episode(self):\n",
    "        S = []\n",
    "        A = []\n",
    "        R = []\n",
    "        self.state = self.environment.reset()\n",
    "        total_episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            s, a = self.choose_action() #overwrite s with difference of new state from the past\n",
    "            s_next, r, done, info = self.environment.step(a)\n",
    "            total_episode_reward += r\n",
    "            S.append(s) # previous state\n",
    "            A.append(a) # action from that state\n",
    "            R.append(r) # reward from action from that state\n",
    "            self.state = s_next\n",
    "            if done:\n",
    "                self.fit(S,A,R) # takes all the items of the episode from S_0,A_1,R_1,...,S_t-1,A_t,R_t\n",
    "                return total_episode_reward\n",
    "            \n",
    "    def training(self):\n",
    "        div = 200\n",
    "        episode_number = 0\n",
    "        group_of_rewards = []\n",
    "        while True:\n",
    "            reward_total_for_episode = self.start_episode()\n",
    "            group_of_rewards.append(reward_total_for_episode)\n",
    "            if episode_number % div == 0 and episode_number != 0:\n",
    "                self.saveweights()\n",
    "                mean_rewards = np.mean(group_of_rewards)\n",
    "                print(\"Mean rewards for \"+ str(div) + \" episodes: \"+ str(mean_rewards))\n",
    "                group_of_rewards = []\n",
    "            episode_number += 1\n",
    "    \n",
    "            \n",
    "    def savemodel(self):\n",
    "        with open('saved_model-'+str(self.weight_i)+'.json', 'w') as modelfile:\n",
    "            modelfile.write(self.model.to_json())\n",
    "            \n",
    "    def saveweights(self):\n",
    "        self.model.save_weights('saved_weights-'+str(self.weight_i)+'.h5', overwrite=True)\n",
    "        \n",
    "    def choose_action(self):\n",
    "        #Probabilistically pick an action given our network outputs.\n",
    "        self.state = self.preprocess(self.state)\n",
    "        s = self.state\n",
    "        if self.prev_processed_state is None:\n",
    "            # prev_processed_state does not exist yet, so we take the difference from 0s but maybe we should be taking the difference from 1s\n",
    "            self.prev_processed_state = np.ones((80, 80, 1), dtype='float32')/2  # Average field\n",
    "        difference_processed = (s - self.prev_processed_state) # fit and predict off the difference values between steps\n",
    "        self.prev_processed_state = s # update for next iteration calculation\n",
    "        # Create a flattened np array of the result of the prediction a 6x1 vector from softmax\n",
    "        self.action_probabilities = self.model.predict(difference_processed.reshape([-1,80,80,1])).flatten()\n",
    "        # Introduce exploration based on a gaussian distribution, we use a very low probability for exploration\n",
    "        # Such as testing if e < -.5,\n",
    "        e = np.random.normal() # range from [~-3.5, ~3.5] where a distribution is according the CDF of the gaussian\n",
    "        if (e < self.random_choice_threshold):\n",
    "            action = np.random.choice(self.environment.action_space.n, 1)[0]\n",
    "        else:\n",
    "            # Choose according to the policy gradient (the prediction's probabilities become the distribution)\n",
    "            action = np.random.choice(self.environment.action_space.n, 1, p=self.action_probabilities)[0]\n",
    "        # Make the step and return the results to the train function (caller)\n",
    "        return difference_processed, action\n",
    "    \n",
    "    def preprocess(self, state):\n",
    "        # take a 210x160x3 image and convert to 210x160x1 as 210*160 length vector flattened F = float32 in dim\n",
    "        # From https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html\n",
    "        #r, g, b = state[:, :, 0], state[:, :, 1], state[:, :, 2]\n",
    "        #F = r * 299.0/1000 + g * 587.0/1000 + b * 114.0/1000\n",
    "        # Now using Karpathy 0/1 conversion of a majority of the surface area of the screen down to 80x80\n",
    "        F = state\n",
    "        F = F[35:195] # crop vertical rows to 160\n",
    "        F = F[::2,::2,0] # downsample by factor of 2 to 80x80 just keeps the first index of the 3rd dim\n",
    "        F[F != 0] = 255 # everything else (paddles, ball) just set to 1\n",
    "        F[F != 255] = 0\n",
    "        return F.reshape([80,80,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_gamma(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    # normalize and return\n",
    "    return (discounted_r - discounted_r.mean()) / (discounted_r.std()+1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 4, 4, 256)\n",
      "(None, 4, 4, 256)\n",
      "(None, 512)\n",
      "(None, 512)\n",
      "Training model without old weights\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 40, 40, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 40, 40, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 40, 40, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 20, 20, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 20, 20, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,746,854\n",
      "Trainable params: 2,743,846\n",
      "Non-trainable params: 3,008\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:171: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rewards for 200 episodes: 1.19900497512\n",
      "Mean rewards for 200 episodes: 1.15\n",
      "Mean rewards for 200 episodes: 1.02\n",
      "Mean rewards for 200 episodes: 1.125\n",
      "Mean rewards for 200 episodes: 1.215\n",
      "Mean rewards for 200 episodes: 1.305\n",
      "Mean rewards for 200 episodes: 1.375\n",
      "Mean rewards for 200 episodes: 0.99\n",
      "Mean rewards for 200 episodes: 1.34\n",
      "Mean rewards for 200 episodes: 1.02\n",
      "Mean rewards for 200 episodes: 1.235\n",
      "Mean rewards for 200 episodes: 1.175\n",
      "Mean rewards for 200 episodes: 1.255\n",
      "Mean rewards for 200 episodes: 1.255\n",
      "Mean rewards for 200 episodes: 1.21\n",
      "Mean rewards for 200 episodes: 1.48\n",
      "Mean rewards for 200 episodes: 1.15\n",
      "Mean rewards for 200 episodes: 1.44\n",
      "Mean rewards for 200 episodes: 1.305\n",
      "Mean rewards for 200 episodes: 1.275\n",
      "Mean rewards for 200 episodes: 1.18\n",
      "Mean rewards for 200 episodes: 1.195\n",
      "Mean rewards for 200 episodes: 1.075\n",
      "Mean rewards for 200 episodes: 1.17\n",
      "Mean rewards for 200 episodes: 1.035\n",
      "Mean rewards for 200 episodes: 1.115\n",
      "Mean rewards for 200 episodes: 1.01\n",
      "Mean rewards for 200 episodes: 1.145\n",
      "Mean rewards for 200 episodes: 1.115\n",
      "Mean rewards for 200 episodes: 1.11\n"
     ]
    }
   ],
   "source": [
    "agent = rl_agent(\n",
    "    learning_rate = .001,\n",
    "    decay_rate = 1e-8,\n",
    "    random_choice_threshold = -2,\n",
    "    optimizer = Adam,\n",
    "    dropout_rate = 0.5,\n",
    "    environment=env,\n",
    "    hidden_units=1024,\n",
    "    epsilon=1e-8,\n",
    "    weight_iteration=2\n",
    "    )\n",
    "agent.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(agent.preprocess(agent.state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200  72  72]\n"
     ]
    }
   ],
   "source": [
    "plt.hist(agent.reward_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
