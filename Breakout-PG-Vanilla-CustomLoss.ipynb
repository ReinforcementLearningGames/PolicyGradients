{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout - Policy Gradient Method w/ Custom Loss\n",
    "\n",
    "## Policy Gradients Vanilla type\n",
    "* Original structure depends heavily on structure of: [Vanilla Policy Gradient](https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb)\n",
    "* Also loss from [Mo Kweon](https://gist.github.com/kkweon/c8d1caabaf7b43317bc8825c226045d2)\n",
    "* Agent structure from [KeRLym](https://github.com/osh/kerlym/blob/master/kerlym/pg.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, convolutional, pooling, Flatten, Dropout, Input, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as Bk\n",
    "from scipy.misc import imread, imshow\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-30 22:34:26,868] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "print(env.observation_space)  # 210v x 160w x 3color Box (sparse tensor!!!)\n",
    "print(env.action_space)       # 6x1 Discrete\n",
    "gamma = 0.99\n",
    "try: \n",
    "    xrange(1)\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rl_agent():\n",
    "    def __init__(self,  \n",
    "                 environment, \n",
    "                 hidden_units = 1024,\n",
    "                 epsilon = 1e-8,                  # Error - error for types of gradient descent or random choice\n",
    "                 learning_rate = .3,\n",
    "                 decay_rate = 1e-4,\n",
    "                 random_choice_threshold = -1,\n",
    "                 optimizer = Adam,\n",
    "                 dropout_rate = 0.5,\n",
    "                 input_dim = (80,80,1),\n",
    "                 output_dim = 6,\n",
    "                 weight_iteration = 1\n",
    "                ):\n",
    "        self.environment = environment\n",
    "        self.hidden_units = hidden_units # hidden neurons per layer (actually double of what gets used)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.optimizer = optimizer(lr=learning_rate, decay=decay_rate)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.input_dim = input_dim\n",
    "        self.render = False\n",
    "        self.prev_processed_state = None\n",
    "        self.random_choice_threshold = random_choice_threshold\n",
    "        self.output_dim = output_dim\n",
    "        self.weight_i = weight_iteration\n",
    "        self.__define_model(self.output_dim)\n",
    "        self.__build_train_fn() # creates self.train_fn()\n",
    "        \n",
    "    def __define_model(self, output_dim):\n",
    "        # Keras vars\n",
    "        Bk.set_learning_phase(1) #set learning phase\n",
    "        self.model = Sequential()\n",
    "        # Sequential model with 4 normalized convolutional layers\n",
    "        self.model.add(convolutional.Conv2D(filters=32, kernel_size = (2,2), strides=(2,2), padding='same', input_shape = (80,80, 1)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(convolutional.Conv2D(filters=64, kernel_size = (2,2), strides=(2,2), padding='valid'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(convolutional.Conv2D(filters=128, kernel_size = (3,3), strides=(2,2), padding='valid'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(convolutional.Conv2D(filters=256, kernel_size = (3,3), strides=(2,2), padding='valid'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        print(self.model.layers[-1].input_shape)\n",
    "        print(self.model.layers[-1].output_shape)\n",
    "        # Model shape following all the stride dimension reductions is:\n",
    "        # 4x4x256\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(self.hidden_units//2))   # Karpathy suggested N=200 in the hidden layer \n",
    "        self.model.add(Dropout(self.dropout_rate))    # But since we are doing dropout to avoid overfitting... increase\n",
    "        self.model.add(BatchNormalization())          # We take a total of 2x fully connected \n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(self.hidden_units//2))\n",
    "        self.model.add(Dropout(self.dropout_rate))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        print(self.model.layers[-1].input_shape)\n",
    "        print(self.model.layers[-1].output_shape)\n",
    "        # Model shape is \n",
    "        self.model.add(Dense(self.output_dim, activation='softmax'))  # Actions are a 6x1 vector\n",
    "        # We use softmax as the last activation to produce a direct parameterization of the policy\n",
    "        # That we can use for probabilities of choosing an action\n",
    "        \n",
    "        # If we are running for the first time, the weights files will not exist\n",
    "        try:\n",
    "             self.model.load_weights('saved_weights-'+str(self.weight_i)+'.h5')\n",
    "        except:\n",
    "            print(\"Training model without old weights\")\n",
    "            \n",
    "        # State space size will be a 210x160x1 dim\n",
    "        self.state = Input(shape=[80,80,1])\n",
    "        \n",
    "        print(self.model.summary())\n",
    "            \n",
    "    def __build_train_fn(self):\n",
    "        # 6x1 tensor\n",
    "        action_prob_placeholder = self.model.output\n",
    "        # 6x1\n",
    "        action_placeholder = Bk.placeholder(shape=(None, self.environment.action_space.n),\n",
    "                                                  name=\"action_vector\")\n",
    "        # nx1\n",
    "        discounted_reward_placeholder = Bk.placeholder(shape=(None,),\n",
    "                                                    name=\"discount_reward\")\n",
    "        # ?? vectorize mult and sum, why not just do action_prob_placeholder.transpose() * action_onehot??\n",
    "        action_prob = Bk.sum(action_prob_placeholder * action_placeholder, axis=1)\n",
    "        # action_prob = action_prob_placeholder.transpose() * action_placeholder\n",
    "        # log of a scalar is a scalar\n",
    "        log_action_prob = Bk.log(action_prob)\n",
    "        \n",
    "        # The alpha for Breakout is always set to 1\n",
    "        # -loss => because Theta = Theta + alpha * gradient * (gamma(r))\n",
    "        loss = -log_action_prob * discounted_reward_placeholder\n",
    "        # Expectation of the loss\n",
    "        loss = Bk.mean(loss)\n",
    "        \n",
    "        # This way we can access the weights\n",
    "        updates = self.optimizer.get_updates(params=self.model.trainable_weights,\n",
    "                                   constraints=[],\n",
    "                                   loss=loss)\n",
    "        \n",
    "        # Feed into the model like the feed_dict in TensorFlow\n",
    "        self.train_fn = Bk.function(inputs=[self.model.input,\n",
    "                                           action_placeholder,\n",
    "                                           discounted_reward_placeholder],\n",
    "                                   outputs=[],\n",
    "                                   updates=updates)\n",
    "        \n",
    "    # For the following functions see Mo Kweon's code from the reference above\n",
    "    # a function which takes the place of the model.fit and implements our training function\n",
    "    def fit(self, S,A,R):\n",
    "        action_onehot = to_categorical(A, num_classes=self.output_dim)\n",
    "        discounted_normed_reward = apply_gamma(R)\n",
    "        self.train_fn([S, action_onehot, discounted_normed_reward]) \n",
    "        # call what was built in build_train_fn which takes as input:\n",
    "        # a list of [input states, categorical actions, and discounted rewards]\n",
    "    \n",
    "    def start_episode(self):\n",
    "        S = []\n",
    "        A = []\n",
    "        R = []\n",
    "        self.state = self.environment.reset()\n",
    "        total_episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            s, a = self.choose_action() #overwrite s with difference of new state from the past\n",
    "            s_next, r, done, info = self.environment.step(a)\n",
    "            total_episode_reward += r\n",
    "            S.append(s) # previous state\n",
    "            A.append(a) # action from that state\n",
    "            R.append(r) # reward from action from that state\n",
    "            self.state = s_next\n",
    "            if done:\n",
    "                self.fit(S,A,R) # takes all the items of the episode from S_0,A_1,R_1,...,S_t-1,A_t,R_t\n",
    "                return total_episode_reward\n",
    "            \n",
    "    def training(self):\n",
    "        div = 200 # Every div episodes output the mean of the games and save the weigths\n",
    "        episode_number = 0\n",
    "        group_of_rewards = []\n",
    "        while True:  # Train forever\n",
    "            reward_total_for_episode = self.start_episode() # training occurs after each episode\n",
    "            group_of_rewards.append(reward_total_for_episode)\n",
    "            if episode_number % div == 0 and episode_number != 0:\n",
    "                self.saveweights()\n",
    "                mean_rewards = np.mean(group_of_rewards)\n",
    "                print(\"Mean rewards for \"+ str(div) + \" episodes: \"+ str(mean_rewards))\n",
    "                group_of_rewards = []\n",
    "            episode_number += 1\n",
    "    \n",
    "            \n",
    "    def savemodel(self):\n",
    "        with open('saved_model-'+str(self.weight_i)+'.json', 'w') as modelfile:\n",
    "            modelfile.write(self.model.to_json())\n",
    "            \n",
    "    def saveweights(self):\n",
    "        self.model.save_weights('saved_weights-'+str(self.weight_i)+'.h5', overwrite=True)\n",
    "        \n",
    "    def choose_action(self):\n",
    "        #Probabilistically pick an action given our network outputs.\n",
    "        self.state = self.preprocess(self.state)\n",
    "        s = self.state\n",
    "        if self.prev_processed_state is None:\n",
    "            # prev_processed_state does not exist yet, so we take the difference from 1/2s\n",
    "            self.prev_processed_state = np.ones((80, 80, 1), dtype='float32')/2  # Average field\n",
    "        difference_processed = (s - self.prev_processed_state) # fit and predict off the difference values between steps\n",
    "        self.prev_processed_state = s # update for next iteration calculation\n",
    "        # Create a flattened np array of the result of the prediction a 6x1 vector from softmax\n",
    "        self.action_probabilities = self.model.predict(difference_processed.reshape([-1,80,80,1])).flatten()\n",
    "        # Introduce exploration based on a gaussian distribution, we use a very low probability for exploration\n",
    "        # Such as testing if e < -2,\n",
    "        e = np.random.normal() # range from [~-4, ~4] where a distribution is according the CDF of the gaussian\n",
    "        if (e < self.random_choice_threshold):\n",
    "            action = np.random.choice(self.environment.action_space.n, 1)[0]\n",
    "        else:\n",
    "            # Choose according to the policy gradient (the prediction's probabilities become the distribution)\n",
    "            action = np.random.choice(self.environment.action_space.n, 1, p=self.action_probabilities)[0]\n",
    "        # Make the step and return the results to the train function (caller)\n",
    "        return difference_processed, action\n",
    "    \n",
    "    def preprocess(self, state):\n",
    "        # take a 210x160x3 image and convert to 210x160x1 as 210*160 length vector flattened F = float32 in dim\n",
    "        # From https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html\n",
    "        #r, g, b = state[:, :, 0], state[:, :, 1], state[:, :, 2]\n",
    "        #F = r * 299.0/1000 + g * 587.0/1000 + b * 114.0/1000\n",
    "        # Now using Karpathy 0/1 conversion of a majority of the surface area of the screen down to 80x80\n",
    "        # See http://karpathy.github.io/2016/05/31/rl/ and https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "        F = state\n",
    "        F = F[35:195] # crop vertical rows to 160 (same as dimension for pong because all Atari games use the same space)\n",
    "        F = F[::2,::2,0] # downsample by factor of 2 to 80x80 just keeps the first index of the 3rd dim\n",
    "        F[F != 0] = 255 # everything else (paddles, ball) just set to 1\n",
    "        F[F != 255] = 0\n",
    "        return F.reshape([80,80,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_gamma(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # This step is gamma^t * R_t and we keep a running sum\n",
    "    for t in reversed(xrange(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    # normalize and return\n",
    "    return (discounted_r - discounted_r.mean()) / (discounted_r.std()+1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 4, 4, 256)\n",
      "(None, 4, 4, 256)\n",
      "(None, 512)\n",
      "(None, 512)\n",
      "Training model without old weights\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 40, 40, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 40, 40, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 40, 40, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 20, 20, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 20, 20, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,746,854\n",
      "Trainable params: 2,743,846\n",
      "Non-trainable params: 3,008\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:171: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rewards for 200 episodes: 1.23383084577\n",
      "Mean rewards for 200 episodes: 1.17\n",
      "Mean rewards for 200 episodes: 1.215\n",
      "Mean rewards for 200 episodes: 1.23\n",
      "Mean rewards for 200 episodes: 1.225\n",
      "Mean rewards for 200 episodes: 1.27\n",
      "Mean rewards for 200 episodes: 1.185\n",
      "Mean rewards for 200 episodes: 1.125\n",
      "Mean rewards for 200 episodes: 1.37\n",
      "Mean rewards for 200 episodes: 1.255\n",
      "Mean rewards for 200 episodes: 1.105\n",
      "Mean rewards for 200 episodes: 1.275\n",
      "Mean rewards for 200 episodes: 1.04\n",
      "Mean rewards for 200 episodes: 1.06\n",
      "Mean rewards for 200 episodes: 1.305\n",
      "Mean rewards for 200 episodes: 1.25\n",
      "Mean rewards for 200 episodes: 1.005\n",
      "Mean rewards for 200 episodes: 1.165\n",
      "Mean rewards for 200 episodes: 1.2\n",
      "Mean rewards for 200 episodes: 1.065\n",
      "Mean rewards for 200 episodes: 1.075\n",
      "Mean rewards for 200 episodes: 1.23\n",
      "Mean rewards for 200 episodes: 1.125\n",
      "Mean rewards for 200 episodes: 1.115\n",
      "Mean rewards for 200 episodes: 0.975\n",
      "Mean rewards for 200 episodes: 1.1\n",
      "Mean rewards for 200 episodes: 1.105\n",
      "Mean rewards for 200 episodes: 1.055\n",
      "Mean rewards for 200 episodes: 1.12\n",
      "Mean rewards for 200 episodes: 1.245\n",
      "Mean rewards for 200 episodes: 1.185\n",
      "Mean rewards for 200 episodes: 1.0\n",
      "Mean rewards for 200 episodes: 1.3\n",
      "Mean rewards for 200 episodes: 1.095\n",
      "Mean rewards for 200 episodes: 1.23\n",
      "Mean rewards for 200 episodes: 1.16\n",
      "Mean rewards for 200 episodes: 1.17\n",
      "Mean rewards for 200 episodes: 1.115\n",
      "Mean rewards for 200 episodes: 1.055\n",
      "Mean rewards for 200 episodes: 1.14\n",
      "Mean rewards for 200 episodes: 1.125\n",
      "Mean rewards for 200 episodes: 1.225\n",
      "Mean rewards for 200 episodes: 1.065\n",
      "Mean rewards for 200 episodes: 1.345\n",
      "Mean rewards for 200 episodes: 1.28\n",
      "Mean rewards for 200 episodes: 1.12\n",
      "Mean rewards for 200 episodes: 1.2\n",
      "Mean rewards for 200 episodes: 1.26\n",
      "Mean rewards for 200 episodes: 1.195\n",
      "Mean rewards for 200 episodes: 1.125\n",
      "Mean rewards for 200 episodes: 1.17\n",
      "Mean rewards for 200 episodes: 1.155\n",
      "Mean rewards for 200 episodes: 1.175\n",
      "Mean rewards for 200 episodes: 1.26\n",
      "Mean rewards for 200 episodes: 1.135\n",
      "Mean rewards for 200 episodes: 1.2\n",
      "Mean rewards for 200 episodes: 1.085\n",
      "Mean rewards for 200 episodes: 1.205\n",
      "Mean rewards for 200 episodes: 1.11\n",
      "Mean rewards for 200 episodes: 1.125\n",
      "Mean rewards for 200 episodes: 1.205\n",
      "Mean rewards for 200 episodes: 1.24\n",
      "Mean rewards for 200 episodes: 1.225\n",
      "Mean rewards for 200 episodes: 1.17\n",
      "Mean rewards for 200 episodes: 1.065\n",
      "Mean rewards for 200 episodes: 1.07\n",
      "Mean rewards for 200 episodes: 1.135\n",
      "Mean rewards for 200 episodes: 1.11\n",
      "Mean rewards for 200 episodes: 1.055\n",
      "Mean rewards for 200 episodes: 1.17\n",
      "Mean rewards for 200 episodes: 1.16\n",
      "Mean rewards for 200 episodes: 1.135\n",
      "Mean rewards for 200 episodes: 1.12\n",
      "Mean rewards for 200 episodes: 1.215\n",
      "Mean rewards for 200 episodes: 1.175\n",
      "Mean rewards for 200 episodes: 1.31\n",
      "Mean rewards for 200 episodes: 1.145\n",
      "Mean rewards for 200 episodes: 1.18\n",
      "Mean rewards for 200 episodes: 1.115\n",
      "Mean rewards for 200 episodes: 1.175\n",
      "Mean rewards for 200 episodes: 0.99\n",
      "Mean rewards for 200 episodes: 1.07\n",
      "Mean rewards for 200 episodes: 1.155\n",
      "Mean rewards for 200 episodes: 1.18\n",
      "Mean rewards for 200 episodes: 1.17\n",
      "Mean rewards for 200 episodes: 1.145\n",
      "Mean rewards for 200 episodes: 1.11\n",
      "Mean rewards for 200 episodes: 1.19\n",
      "Mean rewards for 200 episodes: 1.155\n",
      "Mean rewards for 200 episodes: 1.1\n",
      "Mean rewards for 200 episodes: 1.305\n",
      "Mean rewards for 200 episodes: 1.135\n",
      "Mean rewards for 200 episodes: 1.07\n",
      "Mean rewards for 200 episodes: 1.175\n",
      "Mean rewards for 200 episodes: 1.225\n",
      "Mean rewards for 200 episodes: 1.13\n",
      "Mean rewards for 200 episodes: 1.155\n",
      "Mean rewards for 200 episodes: 1.22\n",
      "Mean rewards for 200 episodes: 1.355\n",
      "Mean rewards for 200 episodes: 0.995\n",
      "Mean rewards for 200 episodes: 1.33\n",
      "Mean rewards for 200 episodes: 1.175\n",
      "Mean rewards for 200 episodes: 1.17\n",
      "Mean rewards for 200 episodes: 1.13\n",
      "Mean rewards for 200 episodes: 1.125\n",
      "Mean rewards for 200 episodes: 1.175\n",
      "Mean rewards for 200 episodes: 1.21\n",
      "Mean rewards for 200 episodes: 1.075\n",
      "Mean rewards for 200 episodes: 1.295\n",
      "Mean rewards for 200 episodes: 1.075\n",
      "Mean rewards for 200 episodes: 1.18\n",
      "Mean rewards for 200 episodes: 1.365\n",
      "Mean rewards for 200 episodes: 1.185\n",
      "Mean rewards for 200 episodes: 1.19\n",
      "Mean rewards for 200 episodes: 1.155\n",
      "Mean rewards for 200 episodes: 1.195\n",
      "Mean rewards for 200 episodes: 1.05\n",
      "Mean rewards for 200 episodes: 1.245\n",
      "Mean rewards for 200 episodes: 1.275\n",
      "Mean rewards for 200 episodes: 1.12\n",
      "Mean rewards for 200 episodes: 1.14\n",
      "Mean rewards for 200 episodes: 0.99\n"
     ]
    }
   ],
   "source": [
    "agent = rl_agent(\n",
    "    learning_rate = .3,\n",
    "    decay_rate = 1e-3,\n",
    "    random_choice_threshold = -1,\n",
    "    optimizer = Adam,\n",
    "    dropout_rate = 0.6,\n",
    "    environment=env,\n",
    "    hidden_units=1024,\n",
    "    epsilon=1e-8,\n",
    "    weight_iteration=3\n",
    "    )\n",
    "agent.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(agent.preprocess(agent.state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200  72  72]\n"
     ]
    }
   ],
   "source": [
    "plt.hist(agent.reward_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
