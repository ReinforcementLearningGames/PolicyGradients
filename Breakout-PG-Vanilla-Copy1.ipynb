{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout - Policy Gradient Method\n",
    "## Policy Gradients Vanilla type\n",
    "* Borrowed heavily from structure of: [Vanilla Policy Gradient](https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb)\n",
    "* In order to determine positive and negative rewards we must associate it with some direction of the pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, convolutional, pooling, Flatten, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as Bk\n",
    "from scipy.misc import imread, imshow\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-06 00:20:58,055] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "Discrete(6)\n",
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "print(env.observation_space)  # 210v x 160w x 3color Box (sparse tensor!!!)\n",
    "print(env.action_space)       # 6x1 Discrete\n",
    "s = env.reset()\n",
    "print(s.shape)\n",
    "gamma = 0.99\n",
    "try: \n",
    "    xrange(1)\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rl_agent():\n",
    "    def __init__(self,  \n",
    "                 environment, \n",
    "                 state_space_size, \n",
    "                 action_space_size,               \n",
    "                 hidden_units = 1096,\n",
    "                 learning_rate = 1e-2,             # Lambda or other for gradient descent\n",
    "                 epsilon = 1e-8,                  # Error - error for types of gradient descent or random choice\n",
    "                 random_choice_threshold = -.72,\n",
    "                 optimizer = Adam,\n",
    "                 dropout_rate = 0.4,\n",
    "                 load_file_weights = None,\n",
    "                 verbose = 0,\n",
    "                 frames = 3\n",
    "                ):\n",
    "        self.environment = environment\n",
    "        self.hidden_units = hidden_units # hidden neurons\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.optimizer = optimizer\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.load_file_weights = load_file_weights\n",
    "        self.verbose = verbose\n",
    "        self.frames = frames\n",
    "        self.input_dim = 210*160\n",
    "        self.prev_processed_state = None\n",
    "        self.past_differences = []\n",
    "        self.render = False\n",
    "        self.random_choice_threshold = random_choice_threshold\n",
    "        \n",
    "    def define_model(self):\n",
    "        # Keras vars\n",
    "        self.model = Sequential()\n",
    "        self.model.add(convolutional.Conv2D(filters=32, kernel_size = (5,5), strides=(1,1), padding='same',\n",
    "                                            input_shape = (3, 210, 160), activation='relu'))\n",
    "        self.model.add(pooling.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "        self.model.add(convolutional.Conv2D(filters=64, kernel_size = (5,5),\n",
    "                                            strides=(1, 1), padding='same', activation='relu'))\n",
    "        self.model.add(pooling.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(self.hidden_units, activation='relu'))   # Karpathy suggested N=200 in the hidden layer\n",
    "        self.model.add(Dropout(self.dropout_rate))      # But since we are doing dropout to avoid overfitting... increase\n",
    "        self.model.add(Dense(6, activation='softmax'))  # Actions are a 6x1 vector\n",
    "        optimizer = self.optimizer(lr=self.learning_rate, beta_1=0.9, beta_2=0.999, epsilon = self.epsilon, decay=0.0)\n",
    "        self.model.compile(\n",
    "            optimizer = optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "             self.model.load_weights('saved_weights5.h5')\n",
    "        except:\n",
    "            print(\"Training model without old weights\")\n",
    "            \n",
    "        # State space size will be a 210x160x1 dim\n",
    "        self.state = Input(shape=self.state_space_size)\n",
    "        self.state1 = Input(shape=self.state_space_size)\n",
    "        #  self.current_action = tf.argmax(self.output_layer,1)\n",
    "        self.reward_tensor = Input(shape=(1,), dtype='float32')\n",
    "        self.action_tensor = Input(shape=(1,), dtype='int32')\n",
    "        \n",
    "        if self.verbose >= 1:\n",
    "            print(self.model.summary())\n",
    "        \n",
    "    def training(self, max_games = 100000):\n",
    "        mean_rewards_list = []\n",
    "        self.state = self.environment.reset()\n",
    "        chosen_vectors, rewards = [], []\n",
    "        running_reward = 0\n",
    "        reward_for_episode = 0\n",
    "        i = 0\n",
    "        while True:\n",
    "            history_element, done = self.choose_action()\n",
    "            chosen_vectors.append(history_element[4])\n",
    "            reward_for_episode += float(history_element[3])\n",
    "            rewards.append(history_element[3])\n",
    "            if done:\n",
    "                i += 1\n",
    "                all_episode_features = np.vstack(agent.past_differences)\n",
    "                all_episode_chosen_vectors = np.vstack(chosen_vectors)\n",
    "                all_episode_rewards = np.vstack(rewards)\n",
    "                discounted_episode_rewards = apply_gamma(all_episode_rewards)\n",
    "                # Standard normal feature scaling \n",
    "                std_dev = np.std(discounted_episode_rewards)+self.epsilon\n",
    "                discounted_episode_rewards = (discounted_episode_rewards - np.mean(discounted_episode_rewards+self.epsilon))/std_dev\n",
    "                all_episode_chosen_vectors = np.multiply(discounted_episode_rewards.transpose(), all_episode_chosen_vectors.transpose()).transpose()\n",
    "                self.model.fit(all_episode_features.reshape([-1, 3, 210,160]), all_episode_chosen_vectors, epochs=3, verbose=0, shuffle=True)\n",
    "                self.prev_processed_state = None\n",
    "                mean_rewards = np.mean(all_episode_rewards)\n",
    "                reward_for_episode = 0\n",
    "                self.state = self.environment.reset()\n",
    "                if i%20 == 0 and not i == 0:\n",
    "                    self.past_differences = []\n",
    "                    chosen_vectors = []\n",
    "                    rewards = []\n",
    "                    self.saveweights()\n",
    "                    mean_rewards_list.append(mean_rewards)\n",
    "                    print(\"MEAN REWARDS:\" + str(mean_rewards))\n",
    "                    \n",
    "    def savemodel(self):\n",
    "        with open('saved_model5.json') as modelfile:\n",
    "            modelfile.write(self.model.to_json())\n",
    "            \n",
    "    def saveweights(self):\n",
    "        self.model.save_weights('saved_weights5.h5', overwrite=True)\n",
    "        \n",
    "    def choose_action(self):\n",
    "        #Probabilistically pick an action given our network outputs.\n",
    "        #s = self.preprocess(self.state)\n",
    "        s = self.state.reshape([3,210,160])\n",
    "        self.prev_processed_state = self.prev_processed_state if self.prev_processed_state is not None else np.zeros((3,210, 160), dtype='float32')\n",
    "        difference_processed = (s - self.prev_processed_state)\n",
    "        self.prev_processed_state = s\n",
    "        # difference_processed = difference_processed.flatten()\n",
    "        self.past_differences.append(difference_processed.flatten())\n",
    "        self.action_probabilities = self.model.predict(difference_processed.reshape([-1,3,210,160])).flatten()\n",
    "        e = np.random.normal()\n",
    "        if (e < self.random_choice_threshold):\n",
    "            action = np.random.choice(self.environment.action_space.n, 1)[0]\n",
    "        else:\n",
    "            action = np.random.choice(self.environment.action_space.n, 1,p=self.action_probabilities/np.sum(self.action_probabilities))[0]\n",
    "        self.state, reward, done, info = self.environment.step(action)\n",
    "        if self.render: self.environment.render()\n",
    "        chosen_vector = np.zeros([self.environment.action_space.n])\n",
    "        chosen_vector[action] = 1\n",
    "        return [s,action, self.state, reward, chosen_vector], done\n",
    "    \n",
    "    def preprocess(self, state):\n",
    "        # take a 210x160x3 image and convert to 210x160x1 as 210*160 length vector flattened F = float32 in dim\n",
    "        # From https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html\n",
    "        r, g, b = state[:, :, 0], state[:, :, 1], state[:, :, 2]\n",
    "        F = r * 299.0/1000 + g * 587.0/1000 + b * 114.0/1000\n",
    "        return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_gamma(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    r = r.flatten()\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 3, 210, 32)        128032    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 105, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 105, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 53, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3392)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1096)              3718728   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 6582      \n",
      "=================================================================\n",
      "Total params: 3,904,606\n",
      "Trainable params: 3,904,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "MEAN REWARDS:0.00725890079502\n",
      "MEAN REWARDS:0.00533638269487\n",
      "MEAN REWARDS:0.00522485538347\n",
      "MEAN REWARDS:0.00512315270936\n",
      "MEAN REWARDS:0.00467574710307\n",
      "MEAN REWARDS:0.00811123986095\n",
      "MEAN REWARDS:0.00629040712913\n",
      "MEAN REWARDS:0.0067760342368\n",
      "MEAN REWARDS:0.00624763347217\n",
      "MEAN REWARDS:0.00655447298494\n",
      "MEAN REWARDS:0.00662251655629\n",
      "MEAN REWARDS:0.00606729178158\n",
      "MEAN REWARDS:0.00508607198748\n",
      "MEAN REWARDS:0.00581184162732\n",
      "MEAN REWARDS:0.00551072623499\n",
      "MEAN REWARDS:0.00549651887138\n",
      "MEAN REWARDS:0.00598109203164\n",
      "MEAN REWARDS:0.00444264943457\n",
      "MEAN REWARDS:0.00704355885079\n",
      "MEAN REWARDS:0.00545073375262\n",
      "MEAN REWARDS:0.00560819957455\n",
      "MEAN REWARDS:0.00463429377393\n",
      "MEAN REWARDS:0.00464834276475\n",
      "MEAN REWARDS:0.00501102425336\n",
      "MEAN REWARDS:0.00554917719097\n",
      "MEAN REWARDS:0.00685693106004\n",
      "MEAN REWARDS:0.00506380392951\n",
      "MEAN REWARDS:0.00639410966867\n",
      "MEAN REWARDS:0.00462869792715\n",
      "MEAN REWARDS:0.0057361376673\n",
      "MEAN REWARDS:0.00506893755069\n",
      "MEAN REWARDS:0.00665065582856\n",
      "MEAN REWARDS:0.00557263643351\n",
      "MEAN REWARDS:0.00654172560113\n",
      "MEAN REWARDS:0.00573504110113\n",
      "MEAN REWARDS:0.00451652638062\n",
      "MEAN REWARDS:0.00505561172902\n",
      "MEAN REWARDS:0.00456258678834\n",
      "MEAN REWARDS:0.00597014925373\n",
      "MEAN REWARDS:0.00701186623517\n",
      "MEAN REWARDS:0.0053412462908\n",
      "MEAN REWARDS:0.00545124167171\n",
      "MEAN REWARDS:0.00557213930348\n",
      "MEAN REWARDS:0.00697714983429\n",
      "MEAN REWARDS:0.00602175602176\n",
      "MEAN REWARDS:0.00572330767713\n",
      "MEAN REWARDS:0.00633161851998\n",
      "MEAN REWARDS:0.00582185134873\n",
      "MEAN REWARDS:0.00607787274454\n",
      "MEAN REWARDS:0.00518754988029\n",
      "MEAN REWARDS:0.00621118012422\n",
      "MEAN REWARDS:0.00515975391943\n",
      "MEAN REWARDS:0.00579884023195\n",
      "MEAN REWARDS:0.00570148645897\n",
      "MEAN REWARDS:0.00701365817645\n",
      "MEAN REWARDS:0.00570753788624\n",
      "MEAN REWARDS:0.00571315996848\n",
      "MEAN REWARDS:0.00491642084562\n",
      "MEAN REWARDS:0.00595016734846\n",
      "MEAN REWARDS:0.00536352800954\n",
      "MEAN REWARDS:0.00543172878816\n",
      "MEAN REWARDS:0.0061410459588\n",
      "MEAN REWARDS:0.00640256102441\n",
      "MEAN REWARDS:0.00631412786109\n",
      "MEAN REWARDS:0.00574143733914\n",
      "MEAN REWARDS:0.00715435521374\n",
      "MEAN REWARDS:0.0067246835443\n",
      "MEAN REWARDS:0.00534759358289\n",
      "MEAN REWARDS:0.0049039640376\n",
      "MEAN REWARDS:0.00596569724087\n",
      "MEAN REWARDS:0.00546226987659\n",
      "MEAN REWARDS:0.00534865293185\n",
      "MEAN REWARDS:0.00562888198758\n",
      "MEAN REWARDS:0.00586832935999\n",
      "MEAN REWARDS:0.00473746545598\n",
      "MEAN REWARDS:0.00402627675355\n",
      "MEAN REWARDS:0.00610084335188\n",
      "MEAN REWARDS:0.00575373993096\n",
      "MEAN REWARDS:0.00545808966862\n",
      "MEAN REWARDS:0.00481830957639\n",
      "MEAN REWARDS:0.00660876132931\n",
      "MEAN REWARDS:0.00442944526471\n",
      "MEAN REWARDS:0.00741947158885\n",
      "MEAN REWARDS:0.00521564694082\n",
      "MEAN REWARDS:0.00626423690205\n",
      "MEAN REWARDS:0.0051495345613\n",
      "MEAN REWARDS:0.00570613409415\n",
      "MEAN REWARDS:0.00501705799719\n",
      "MEAN REWARDS:0.00567625758465\n",
      "MEAN REWARDS:0.00461106655974\n",
      "MEAN REWARDS:0.00537634408602\n",
      "MEAN REWARDS:0.00523053080201\n",
      "MEAN REWARDS:0.00556881463803\n",
      "MEAN REWARDS:0.00438871473354\n",
      "MEAN REWARDS:0.00734753857458\n",
      "MEAN REWARDS:0.00398155909472\n",
      "MEAN REWARDS:0.00550314465409\n",
      "MEAN REWARDS:0.00524068322981\n",
      "MEAN REWARDS:0.00631618056846\n",
      "MEAN REWARDS:0.00547945205479\n",
      "MEAN REWARDS:0.00431211498973\n",
      "MEAN REWARDS:0.00641267446247\n",
      "MEAN REWARDS:0.00536352800954\n",
      "MEAN REWARDS:0.00455958549223\n",
      "MEAN REWARDS:0.00669600153051\n",
      "MEAN REWARDS:0.0043997485858\n",
      "MEAN REWARDS:0.00721113533456\n",
      "MEAN REWARDS:0.00659066619166\n",
      "MEAN REWARDS:0.00571766561514\n",
      "MEAN REWARDS:0.00586343862304\n",
      "MEAN REWARDS:0.00562124442721\n",
      "MEAN REWARDS:0.00618596559057\n",
      "MEAN REWARDS:0.00443280274028\n",
      "MEAN REWARDS:0.0067152724482\n",
      "MEAN REWARDS:0.00405679513185\n",
      "MEAN REWARDS:0.00625690099374\n",
      "MEAN REWARDS:0.00647619047619\n",
      "MEAN REWARDS:0.00656934306569\n",
      "MEAN REWARDS:0.00428540818513\n",
      "MEAN REWARDS:0.00734908136483\n",
      "MEAN REWARDS:0.00425441395448\n",
      "MEAN REWARDS:0.0054593373494\n",
      "MEAN REWARDS:0.00543056633049\n",
      "MEAN REWARDS:0.00634802091113\n",
      "MEAN REWARDS:0.00694571376348\n",
      "MEAN REWARDS:0.0046408393866\n",
      "MEAN REWARDS:0.00417885499373\n",
      "MEAN REWARDS:0.00658142158706\n",
      "MEAN REWARDS:0.00686664257318\n",
      "MEAN REWARDS:0.00632911392405\n",
      "MEAN REWARDS:0.00403654132144\n",
      "MEAN REWARDS:0.00587789154342\n",
      "MEAN REWARDS:0.00600487896416\n",
      "MEAN REWARDS:0.00465608465608\n",
      "MEAN REWARDS:0.00445795339412\n",
      "MEAN REWARDS:0.00698080279232\n",
      "MEAN REWARDS:0.0060263653484\n",
      "MEAN REWARDS:0.00609756097561\n",
      "MEAN REWARDS:0.003972402258\n",
      "MEAN REWARDS:0.00485240598463\n",
      "MEAN REWARDS:0.00591151792525\n",
      "MEAN REWARDS:0.00497017892644\n",
      "MEAN REWARDS:0.00431122972696\n",
      "MEAN REWARDS:0.00446523495641\n",
      "MEAN REWARDS:0.00503018108652\n",
      "MEAN REWARDS:0.00543587678679\n",
      "MEAN REWARDS:0.0065334358186\n",
      "MEAN REWARDS:0.00412201154163\n",
      "MEAN REWARDS:0.006007133471\n",
      "MEAN REWARDS:0.00408953938872\n",
      "MEAN REWARDS:0.0054012345679\n",
      "MEAN REWARDS:0.00593074421274\n",
      "MEAN REWARDS:0.0066830812522\n",
      "MEAN REWARDS:0.00624082232012\n",
      "MEAN REWARDS:0.00537313432836\n",
      "MEAN REWARDS:0.00428134556575\n",
      "MEAN REWARDS:0.00538062973296\n",
      "MEAN REWARDS:0.00505561172902\n",
      "MEAN REWARDS:0.00547088706526\n",
      "MEAN REWARDS:0.00608712193266\n",
      "MEAN REWARDS:0.00595459620394\n",
      "MEAN REWARDS:0.00569337128914\n",
      "MEAN REWARDS:0.00427263479145\n",
      "MEAN REWARDS:0.00539676194283\n",
      "MEAN REWARDS:0.00568720379147\n",
      "MEAN REWARDS:0.00492902208202\n",
      "MEAN REWARDS:0.00421141292904\n",
      "MEAN REWARDS:0.00697050938338\n",
      "MEAN REWARDS:0.00602836879433\n",
      "MEAN REWARDS:0.00644567219153\n",
      "MEAN REWARDS:0.00548112058465\n",
      "MEAN REWARDS:0.00631941784757\n",
      "MEAN REWARDS:0.00695931477516\n",
      "MEAN REWARDS:0.00538707102953\n",
      "MEAN REWARDS:0.00500901622921\n",
      "MEAN REWARDS:0.00564765895427\n",
      "MEAN REWARDS:0.00653594771242\n",
      "MEAN REWARDS:0.00466910272026\n",
      "MEAN REWARDS:0.0064838829196\n",
      "MEAN REWARDS:0.00536779324056\n",
      "MEAN REWARDS:0.00515975391943\n",
      "MEAN REWARDS:0.00589535740604\n",
      "MEAN REWARDS:0.00846846846847\n",
      "MEAN REWARDS:0.00606891151135\n",
      "MEAN REWARDS:0.00626780626781\n",
      "MEAN REWARDS:0.00592960979342\n",
      "MEAN REWARDS:0.00454921422663\n",
      "MEAN REWARDS:0.00527034940465\n",
      "MEAN REWARDS:0.0054882664648\n",
      "MEAN REWARDS:0.0045054269916\n",
      "MEAN REWARDS:0.00660007135212\n",
      "MEAN REWARDS:0.00539811066127\n",
      "MEAN REWARDS:0.00639619883041\n",
      "MEAN REWARDS:0.00535501785006\n",
      "MEAN REWARDS:0.00505050505051\n",
      "MEAN REWARDS:0.00496721637195\n",
      "MEAN REWARDS:0.00522928399035\n",
      "MEAN REWARDS:0.00576368876081\n",
      "MEAN REWARDS:0.00403397027601\n",
      "MEAN REWARDS:0.00446428571429\n",
      "MEAN REWARDS:0.00489596083231\n",
      "MEAN REWARDS:0.00626713670192\n",
      "MEAN REWARDS:0.00556107249255\n",
      "MEAN REWARDS:0.00466793974114\n",
      "MEAN REWARDS:0.0073913827294\n",
      "MEAN REWARDS:0.00601008142691\n",
      "MEAN REWARDS:0.00685736677116\n",
      "MEAN REWARDS:0.00524781341108\n",
      "MEAN REWARDS:0.00716121924348\n",
      "MEAN REWARDS:0.00627137970353\n",
      "MEAN REWARDS:0.00622368661907\n",
      "MEAN REWARDS:0.0054347826087\n",
      "MEAN REWARDS:0.00619951155364\n",
      "MEAN REWARDS:0.00446156966133\n",
      "MEAN REWARDS:0.00674075423574\n",
      "MEAN REWARDS:0.00659418998396\n",
      "MEAN REWARDS:0.00431300061614\n",
      "MEAN REWARDS:0.00518931385739\n",
      "MEAN REWARDS:0.00632322856611\n",
      "MEAN REWARDS:0.00594574507618\n",
      "MEAN REWARDS:0.00662126371547\n",
      "MEAN REWARDS:0.00597572362278\n",
      "MEAN REWARDS:0.00648330058939\n",
      "MEAN REWARDS:0.00515157519318\n",
      "MEAN REWARDS:0.00634091756807\n",
      "MEAN REWARDS:0.00507305194805\n",
      "MEAN REWARDS:0.00552922590837\n",
      "MEAN REWARDS:0.00469116497263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN REWARDS:0.00562136117246\n",
      "MEAN REWARDS:0.00555555555556\n",
      "MEAN REWARDS:0.00428658909982\n",
      "MEAN REWARDS:0.00616332819723\n",
      "MEAN REWARDS:0.00537118741608\n",
      "MEAN REWARDS:0.00702070207021\n",
      "MEAN REWARDS:0.00544396470809\n",
      "MEAN REWARDS:0.00678692623683\n",
      "MEAN REWARDS:0.00607287449393\n",
      "MEAN REWARDS:0.00768156424581\n",
      "MEAN REWARDS:0.00554387306442\n",
      "MEAN REWARDS:0.0061773255814\n",
      "MEAN REWARDS:0.00441412520064\n",
      "MEAN REWARDS:0.00419903422213\n",
      "MEAN REWARDS:0.00506996552423\n",
      "MEAN REWARDS:0.0065717415115\n",
      "MEAN REWARDS:0.00573122529644\n",
      "MEAN REWARDS:0.00648628613788\n",
      "MEAN REWARDS:0.00601970083911\n",
      "MEAN REWARDS:0.00516647531573\n",
      "MEAN REWARDS:0.00561692566935\n",
      "MEAN REWARDS:0.00486125177233\n",
      "MEAN REWARDS:0.00519480519481\n",
      "MEAN REWARDS:0.00478373529998\n",
      "MEAN REWARDS:0.00563654033042\n",
      "MEAN REWARDS:0.00604996096799\n",
      "MEAN REWARDS:0.00628092881614\n",
      "MEAN REWARDS:0.00683945284377\n",
      "MEAN REWARDS:0.0044998977296\n",
      "MEAN REWARDS:0.00646950092421\n",
      "MEAN REWARDS:0.00676397294411\n",
      "MEAN REWARDS:0.00620767494357\n",
      "MEAN REWARDS:0.00552380952381\n",
      "MEAN REWARDS:0.0059558117195\n",
      "MEAN REWARDS:0.00539936697077\n",
      "MEAN REWARDS:0.00606060606061\n",
      "MEAN REWARDS:0.00616792678074\n",
      "MEAN REWARDS:0.00517001391927\n",
      "MEAN REWARDS:0.00656455142232\n",
      "MEAN REWARDS:0.00419375131055\n",
      "MEAN REWARDS:0.00548159749413\n",
      "MEAN REWARDS:0.00773438973779\n",
      "MEAN REWARDS:0.00584354382658\n",
      "MEAN REWARDS:0.00578815357901\n",
      "MEAN REWARDS:0.00608732157851\n",
      "MEAN REWARDS:0.00594211232509\n",
      "MEAN REWARDS:0.00559197840339\n",
      "MEAN REWARDS:0.00552268244576\n",
      "MEAN REWARDS:0.00593074421274\n",
      "MEAN REWARDS:0.00757439134355\n",
      "MEAN REWARDS:0.00603511338698\n",
      "MEAN REWARDS:0.00521251002406\n",
      "MEAN REWARDS:0.00569073783359\n",
      "MEAN REWARDS:0.00699021370082\n",
      "MEAN REWARDS:0.00596421471173\n",
      "MEAN REWARDS:0.00487329434698\n",
      "MEAN REWARDS:0.00712458896602\n",
      "MEAN REWARDS:0.00371828521435\n",
      "MEAN REWARDS:0.00626612605971\n",
      "MEAN REWARDS:0.00593237097093\n",
      "MEAN REWARDS:0.00694578429481\n",
      "MEAN REWARDS:0.00483675937122\n",
      "MEAN REWARDS:0.00519858598461\n",
      "MEAN REWARDS:0.00664630860428\n",
      "MEAN REWARDS:0.00480288172904\n",
      "MEAN REWARDS:0.005453837164\n",
      "MEAN REWARDS:0.00742442990985\n",
      "MEAN REWARDS:0.00543689320388\n",
      "MEAN REWARDS:0.00626266347394\n",
      "MEAN REWARDS:0.00748216460762\n",
      "MEAN REWARDS:0.00529411764706\n",
      "MEAN REWARDS:0.00518858511275\n",
      "MEAN REWARDS:0.00615502981343\n",
      "MEAN REWARDS:0.00528169014085\n",
      "MEAN REWARDS:0.00488499898229\n",
      "MEAN REWARDS:0.00467653936087\n",
      "MEAN REWARDS:0.00557213930348\n",
      "MEAN REWARDS:0.00726121765034\n",
      "MEAN REWARDS:0.00692880651308\n",
      "MEAN REWARDS:0.00462404503418\n",
      "MEAN REWARDS:0.00644794234781\n",
      "MEAN REWARDS:0.0036093418259\n",
      "MEAN REWARDS:0.00455085081124\n",
      "MEAN REWARDS:0.0063009636768\n",
      "MEAN REWARDS:0.00703689615824\n",
      "MEAN REWARDS:0.00526648409522\n",
      "MEAN REWARDS:0.00531045751634\n",
      "MEAN REWARDS:0.00513833992095\n",
      "MEAN REWARDS:0.00676848316557\n",
      "MEAN REWARDS:0.00577034045009\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3a5ca7b417f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     verbose=1)\n\u001b[0;32m      9\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-bd910c83ab45>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(self, max_games)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mdiscounted_episode_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiscounted_episode_rewards\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscounted_episode_rewards\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mstd_dev\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0mall_episode_chosen_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscounted_episode_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_episode_chosen_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_episode_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m210\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_episode_chosen_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_processed_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mmean_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_episode_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1141\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1142\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2103\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2104\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[0mrun_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    784\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = rl_agent(\n",
    "    environment=env,\n",
    "    state_space_size=(3,210,160),\n",
    "    action_space_size=6,\n",
    "    hidden_units=1096,\n",
    "    epsilon=1e-8, \n",
    "    optimizer=Adam,\n",
    "    verbose=1)\n",
    "agent.define_model()\n",
    "agent.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
