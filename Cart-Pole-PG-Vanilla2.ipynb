{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole - Policy Gradient Method\n",
    "## Policy Gradients Vanilla type\n",
    "* Borrowed heavily from structure of: [Vanilla Policy Gradient](https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb)\n",
    "* In order to determine positive and negative rewards we must associate it with some direction of the pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim  # Easy model building, not quite sure yet how to build nodes by hand\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-22 22:04:43,472] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(env.observation_space)  # 4x1 Box (vector-like)\n",
    "print(env.action_space)       # 2x1 Discrete\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rl_agent():\n",
    "    def __init__(self,  \n",
    "                 environment, \n",
    "                 state_space_size, \n",
    "                 action_space_size,         # For CartPole-v0\n",
    "                 history_size,\n",
    "                 learning_rate,             # Lambda or other for gradient descent\n",
    "                 epsilon,                   # Error - error for types of gradient descent or random choice\n",
    "                 gradient_descent_function):\n",
    "        self.environment = environment\n",
    "        self.history_size = history_size\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.GDFunction = gradient_descent_function\n",
    "        \n",
    "        # TF Vars\n",
    "        self.state = tf.placeholder(shape=[None,self.state_space_size], dtype=tf.float32)\n",
    "        \n",
    "        # ReLu is default\n",
    "        self.hidden_layer = slim.fully_connected(self.state,\n",
    "                                                 self.history_size,\n",
    "                                                 biases_initializer=None,\n",
    "                                                 activation_fn=tf.nn.relu)\n",
    "        # Output with softmax (only two possible choices, but faster to do softmax)\n",
    "        self.output_layer = slim.fully_connected(self.hidden_layer,\n",
    "                                                 self.action_space_size,\n",
    "                                                 biases_initializer=None,\n",
    "                                                 activation_fn=tf.nn.softmax)\n",
    "        self.current_action = tf.argmax(self.output_layer,1)\n",
    "        self.reward_tensor = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_tensor = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        # Range from 0 to the output dimension -- an index range [ 0, 1, 2...\n",
    "        output_range = tf.range(0, tf.shape(self.output_layer)[0])\n",
    "        # Basically this outputs the action \n",
    "        self.indexes = output_range * tf.shape(self.output_layer)[1] + self.action_tensor\n",
    "        # Formed an action tensor \n",
    "        self.output_tensor = tf.gather(tf.reshape(self.output_layer, [-1]), self.indexes)\n",
    "        # Basically a spread and gather according to the index yielding output\n",
    "        # which will be like y * y^ using the reduce mean here so we can reduce to 1x1\n",
    "        self.loss_function = -tf.reduce_mean(tf.log(self.output_tensor)*self.reward_tensor)\n",
    "        \n",
    "        self.trainable_variables = tf.trainable_variables()\n",
    "        \n",
    "        self.gradient_holders = []\n",
    "        # Each trainable variable is a partial derivative, but here they are just placeholders\n",
    "        for idx,var in enumerate(self.trainable_variables):\n",
    "            temp = tf.placeholder(tf.float32, name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(temp)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss_function, self.trainable_variables)\n",
    "        \n",
    "        optimizer = self.GDFunction(learning_rate=self.learning_rate)\n",
    "        self.updated_weights = optimizer.apply_gradients(zip(self.gradient_holders,self.trainable_variables))\n",
    "    \n",
    "    def set_up_gradient_holder(self, session):\n",
    "        self.grad_buffer=session.run(self.trainable_variables)\n",
    "        for index, gradient in enumerate(self.grad_buffer):\n",
    "            self.grad_buffer[index] = gradient * 0 # here we just want the sizes\n",
    "\n",
    "    def update_gradients(self, session, feed_dict):\n",
    "        tempgradients = session.run(self.gradients, feed_dict=feed_dict)\n",
    "        for index, gradient in enumerate(tempgradients):\n",
    "            self.grad_buffer[index] -= gradient\n",
    "\n",
    "    def update_batch_gradients(self, session):\n",
    "        feed_dict = dictionary = dict(zip(self.gradient_holders, self.grad_buffer))\n",
    "        _ = session.run(self.updated_weights, feed_dict=feed_dict)\n",
    "        for index, gradient in enumerate(self.grad_buffer):\n",
    "            self.grad_buffer[index] = gradient * 0\n",
    "        \n",
    "    def choose_action(self, session, s):\n",
    "        #Probabilistically pick an action given our network outputs.\n",
    "        action_probabilities = session.run(self.output_layer,feed_dict={self.state:[s]})\n",
    "        action = np.random.choice(action_probabilities[0],p=action_probabilities[0])\n",
    "        action = np.argmax([action_probabilities == action])\n",
    "        new_state, reward, done, info = self.environment.step(action) \n",
    "        return [s,action, new_state, reward], done\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gamma(list_of_rewards):\n",
    "    index = np.arange(len(list_of_rewards))\n",
    "    gammas = np.power(gamma, index) # Awjuliani misses this step\n",
    "    vectorized_mult = np.multiply(gammas, list_of_rewards)\n",
    "    return vectorized_mult\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n",
      "24.67\n",
      "35.93\n",
      "40.8\n",
      "50.63\n",
      "54.08\n",
      "68.49\n",
      "72.35\n",
      "81.17\n",
      "115.07\n",
      "128.46\n",
      "151.35\n",
      "150.37\n",
      "153.82\n",
      "152.45\n",
      "147.16\n",
      "145.61\n",
      "159.52\n",
      "165.11\n",
      "180.95\n",
      "187.16\n",
      "180.48\n",
      "185.74\n",
      "183.32\n",
      "185.3\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "repeats = 999\n",
    "max_games = 5000\n",
    "agent = rl_agent(\n",
    "    environment=env,\n",
    "    state_space_size=4,\n",
    "    action_space_size=2,\n",
    "    history_size=8,\n",
    "    learning_rate=0.01,\n",
    "    epsilon=1e-8, \n",
    "    gradient_descent_function=tf.train.AdamOptimizer)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    init = session.run(init)\n",
    "    i = 0\n",
    "    rewards = []\n",
    "    time_above_ground = []\n",
    "    agent.set_up_gradient_holder(session)\n",
    "    while i < max_games:\n",
    "        state = env.reset()\n",
    "        reward_for_game = 0\n",
    "        history = [] # 4 items from past\n",
    "        for j in xrange(repeats):\n",
    "            history_element, done = agent.choose_action(session, state)\n",
    "            history.append(history_element)\n",
    "            state = history_element[-2]\n",
    "            reward_for_game += history_element[-1]\n",
    "            if done:\n",
    "                history = np.array(history)\n",
    "                history[:,3] = apply_gamma(history[:,3])\n",
    "                feed_dict = {agent.reward_tensor: history[:,3],\n",
    "                             agent.action_tensor: history[:,1],\n",
    "                             agent.state: np.vstack(history[:,0])}\n",
    "                agent.update_gradients(session, feed_dict)\n",
    "                if i % 5 == 0 and i != 0:\n",
    "                    agent.update_batch_gradients(session)\n",
    "                rewards.append(reward_for_game)\n",
    "                time_above_ground.append(j)\n",
    "                break\n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(rewards[-100:]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
