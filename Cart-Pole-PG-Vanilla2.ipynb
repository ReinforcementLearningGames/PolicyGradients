{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Cart Pole - Policy Gradient Method\n",
    "## Policy Gradients Vanilla type\n",
    "* Borrowed heavily from structure of: [Vanilla Policy Gradient](https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb)\n",
    "* In order to determine positive and negative rewards we must associate it with some direction of the pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim  # Easy model building, not quite sure yet how to build nodes by hand\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-17 14:34:25,147] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\nDiscrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(env.observation_space)  # 4x1 Box (vector-like)\n",
    "print(env.action_space)       # 2x1 Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-9ed75403984c>, line 21)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9ed75403984c>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    self.state = tf.placeholder(shape=[,self.state_space_size], dtype=tf.float32)\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "class rl_agent():\n",
    "    def __init__(self,  \n",
    "                 environment, \n",
    "                 state_space_size, \n",
    "                 action_space_size,         # For CartPole-v0\n",
    "                 history_size,\n",
    "                 learning_rate,             # Lambda or other for gradient descent\n",
    "                 epsilon,                   # Error - error for types of gradient descent or random choice\n",
    "                 gradient_descent_function):\n",
    "        tf.reset_default_graph()\n",
    "        self.history_size = history_size\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.GDFunction = gradient_descent_function\n",
    "        \n",
    "        # TF Vars\n",
    "        self.gradients = [] \n",
    "        self.state = tf.placeholder(shape=[,self.state_space_size], dtype=tf.float32)\n",
    "        \n",
    "        # ReLu is default\n",
    "        self.hidden_layer = slim.fully_connected(self.state,\n",
    "                                                 self.history_size,\n",
    "                                                 biases_initializer=None)\n",
    "        # Output with softmax (only two possible choices, but faster to do softmax)\n",
    "        self.output_layer = slim.fully_connected(self.hidden_layer,\n",
    "                                                 self.action_space_size,\n",
    "                                                 biases_initializer=None,\n",
    "                                                 activation_fn=tf.nn.softmax())\n",
    "        self.current_action = tf.argmax(self.output_layer,1)\n",
    "        self.reward_tensor = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_tensor = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        # Range from 0 to the output dimension -- an index range [ 0, 1, 2...\n",
    "        output_range = tf.range(0, tf.shape(self.output_layer)[0])\n",
    "        # Basically this outputs the action \n",
    "        self.indexes = output_range * tf.shape(self.output_layer)[1] + self.action_tensor\n",
    "        # Formed an action tensor \n",
    "        self.output_tensor = tf.gather(tf.reshape(self.output_layer, [-1]), self.indexes)\n",
    "        # Basically a spread and gather according to the index yielding output\n",
    "        # which will be like y * y^ using the reduce mean here so we can reduce to 1x1\n",
    "        self.loss_function = -tf.reduce_mean(tf.log(self.output_tensor)*self.reward_tensor)\n",
    "        \n",
    "        self.trainable_variables = tf.trainable_variables()\n",
    "        \n",
    "        self.gradient_holders = []\n",
    "        # Each trainable variable is a partial derivative, but here they are just placeholders\n",
    "        for idx,var in enumerate(self.trainable_variables):\n",
    "            temp = tf.placeholder(tf.float32, name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(temp)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss_function, self.trainable_variables)\n",
    "        \n",
    "        optimizer = self.GDFunction(learning_rate=self.learning_rate)\n",
    "        self.updated_weights = optimizer.apply_gradients(zip(self.gradient_holders,trainable_variables))\n",
    "        \n",
    "    def choose_action(self, session, s):\n",
    "        return session.run(self.output_layer, feed_dict={self.state:[s]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-96cdcad7ea2e>, line 10)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-96cdcad7ea2e>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    self.session.run(self.init)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "agent = rl_agent(\n",
    "    environment=env,\n",
    "    state_space_size=4,\n",
    "    action_space_size=2,\n",
    "    learning_rate=0.001,\n",
    "    epsilon=1e-8, \n",
    "    gradient_descent_function=tf.train.AdamOptimizer)\n",
    "\n",
    "    init = \n",
    "        self.session.run(self.init)\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        self.gradients = self.session.run(trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}