{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Cart Pole - Policy Gradient Method\n",
    "## Policy Gradients Vanilla type\n",
    "* Borrowed heavily from structure of: [Vanilla Policy Gradient](https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb)\n",
    "* In order to determine positive and negative rewards we must associate it with some direction of the pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim  # Easy model building, not quite sure yet how to build nodes by hand\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-22 20:38:09,045] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(env.observation_space)  # 4x1 Box (vector-like)\n",
    "print(env.action_space)       # 2x1 Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class rl_agent():\n",
    "    def __init__(self,  \n",
    "                 environment, \n",
    "                 state_space_size, \n",
    "                 action_space_size,         # For CartPole-v0\n",
    "                 history_size,\n",
    "                 learning_rate,             # Lambda or other for gradient descent\n",
    "                 epsilon,                   # Error - error for types of gradient descent or random choice\n",
    "                 gradient_descent_function):\n",
    "        self.environment = environment\n",
    "        self.history_size = history_size\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.GDFunction = gradient_descent_function\n",
    "        \n",
    "        # TF Vars\n",
    "        self.state = tf.placeholder(shape=[None,self.state_space_size], dtype=tf.float32)\n",
    "        \n",
    "        # ReLu is default\n",
    "        self.hidden_layer = slim.fully_connected(self.state,\n",
    "                                                 self.history_size,\n",
    "                                                 biases_initializer=None,\n",
    "                                                 activation_fn=tf.nn.relu)\n",
    "        # Output with softmax (only two possible choices, but faster to do softmax)\n",
    "        self.output_layer = slim.fully_connected(self.hidden_layer,\n",
    "                                                 self.action_space_size,\n",
    "                                                 biases_initializer=None,\n",
    "                                                 activation_fn=tf.nn.softmax)\n",
    "        self.current_action = tf.argmax(self.output_layer,1)\n",
    "        self.reward_tensor = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_tensor = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        # Range from 0 to the output dimension -- an index range [ 0, 1, 2...\n",
    "        output_range = tf.range(0, tf.shape(self.output_layer)[0])\n",
    "        # Basically this outputs the action \n",
    "        self.indexes = output_range * tf.shape(self.output_layer)[1] + self.action_tensor\n",
    "        # Formed an action tensor \n",
    "        self.output_tensor = tf.gather(tf.reshape(self.output_layer, [-1]), self.indexes)\n",
    "        # Basically a spread and gather according to the index yielding output\n",
    "        # which will be like y * y^ using the reduce mean here so we can reduce to 1x1\n",
    "        self.loss_function = -tf.reduce_mean(tf.log(self.output_tensor)*self.reward_tensor)\n",
    "        \n",
    "        self.trainable_variables = tf.trainable_variables()\n",
    "        \n",
    "        self.gradient_holders = []\n",
    "        # Each trainable variable is a partial derivative, but here they are just placeholders\n",
    "        for idx,var in enumerate(self.trainable_variables):\n",
    "            temp = tf.placeholder(tf.float32, name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(temp)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss_function, self.trainable_variables)\n",
    "        \n",
    "        optimizer = self.GDFunction(learning_rate=self.learning_rate)\n",
    "        self.updated_weights = optimizer.apply_gradients(zip(self.gradient_holders,self.trainable_variables))\n",
    "    \n",
    "    def set_up_gradient_holder(self, session):\n",
    "        self.grad_buffer=session.run(self.trainable_variables)\n",
    "        for index, gradient in enumerate(self.grad_buffer):\n",
    "            gradBuffer[index] = grad * 0 # here we just want the sizes\n",
    "    \n",
    "    def choose_action(self, session, s):\n",
    "        #Probabilistically pick an action given our network outputs.\n",
    "            action_probabilities = session.run(self.output_layer,feed_dict={self.state:[s]})\n",
    "            action = np.random.choice(action_probabilities[0],p=action_probabilities[0])\n",
    "            action = np.argmax([action_probabilities == action])\n",
    "            new_state, reward, done, info = self.environment.step(a) \n",
    "            return [s,action, new_state, reward], done\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def apply_gamma(list_of_rewards):\n",
    "    gammas = np.ones_like(list_of_rewards)\n",
    "    index = np.range(len(list_of_rewards))\n",
    "    gammas = gammas * np.power(gamma, index)\n",
    "    print gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "rl_agent instance has no attribute 'training_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ff3cb38340fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtime_above_ground\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_up_gradient_holder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_games\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-070eabc6a2ec>\u001b[0m in \u001b[0;36mset_up_gradient_holder\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_up_gradient_holder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mgradBuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# here we just want the sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: rl_agent instance has no attribute 'training_variables'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "init = tf.global_variables_initializer()\n",
    "repeats = 99\n",
    "max_games = 500\n",
    "agent = rl_agent(\n",
    "    environment=env,\n",
    "    state_space_size=4,\n",
    "    action_space_size=2,\n",
    "    history_size=4,\n",
    "    learning_rate=0.001,\n",
    "    epsilon=1e-8, \n",
    "    gradient_descent_function=tf.train.AdamOptimizer)\n",
    "with tf.Session() as session:\n",
    "    init = session.run(init)\n",
    "    i = 0\n",
    "    rewards = []\n",
    "    time_above_ground = []\n",
    "    agent.set_up_gradient_holder(session)\n",
    "    while i < max_games:\n",
    "        state = env.reset()\n",
    "        reward_for_game = 0\n",
    "        history = [] # 4 items from past\n",
    "        for j in xrange(repeats):\n",
    "            history_element, done = agent.choose_action(session, s)\n",
    "            history.append(history_element)\n",
    "            s = history_element[-2]\n",
    "            reward_for_game += history_element[-1]\n",
    "            if done:\n",
    "                history = np.array(history)\n",
    "                history[:,3] = apply_gamma(history[:,3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
