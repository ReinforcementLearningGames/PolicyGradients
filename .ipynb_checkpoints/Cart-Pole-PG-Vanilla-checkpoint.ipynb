{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole - Policy Gradient Method\n",
    "## Policy Gradients Vanilla type\n",
    "* Borrowed heavily from structure of: [Vanilla Policy Gradient](https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb)\n",
    "* In order to determine positive and negative rewards we must associate it with some direction of the pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim  # Easy model building, not quite sure yet how to build nodes by hand\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-23 17:05:07,755] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(env.observation_space)  # 4x1 Box (vector-like)\n",
    "print(env.action_space)       # 2x1 Discrete\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rl_agent():\n",
    "    def __init__(self,  \n",
    "                 environment, \n",
    "                 state_space_size, \n",
    "                 action_space_size,         # For CartPole-v0\n",
    "                 hidden_units,\n",
    "                 learning_rate,             # Lambda or other for gradient descent\n",
    "                 epsilon,                   # Error - error for types of gradient descent or random choice\n",
    "                 gradient_descent_function):\n",
    "        self.environment = environment\n",
    "        self.hidden_units = hidden_units\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.GDFunction = gradient_descent_function\n",
    "        \n",
    "        # TF Vars\n",
    "        self.state = tf.placeholder(shape=[None,self.state_space_size], dtype=tf.float32)\n",
    "        \n",
    "        # ReLu is default\n",
    "        self.hidden_layer = slim.fully_connected(self.state,\n",
    "                                                 self.hidden_units,\n",
    "                                                 biases_initializer=None,\n",
    "                                                 activation_fn=tf.nn.relu)\n",
    "        # Output with softmax (only two possible choices, but faster to do softmax)\n",
    "        self.output_layer = slim.fully_connected(self.hidden_layer,\n",
    "                                                 self.action_space_size,\n",
    "                                                 biases_initializer=None,\n",
    "                                                 activation_fn=tf.nn.softmax)\n",
    "        self.current_action = tf.argmax(self.output_layer,1)\n",
    "        self.reward_tensor = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_tensor = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        # Range from 0 to the output dimension -- an index range [ 0, 1, 2...\n",
    "        output_range = tf.range(0, tf.shape(self.output_layer)[0])\n",
    "        # Basically this outputs the action \n",
    "        self.indexes = output_range * tf.shape(self.output_layer)[1] + self.action_tensor\n",
    "        # Formed an action tensor \n",
    "        self.output_tensor = tf.gather(tf.reshape(self.output_layer, [-1]), self.indexes)\n",
    "        # Basically a spread and gather according to the index yielding output\n",
    "        # which will be like y * y^ using the reduce mean here so we can reduce to 1x1\n",
    "        self.loss_function = -tf.reduce_mean(tf.log(self.output_tensor)*self.reward_tensor)\n",
    "        \n",
    "        self.trainable_variables = tf.trainable_variables()\n",
    "        \n",
    "        self.gradient_holders = []\n",
    "        # Each trainable variable is a partial derivative, but here they are just placeholders\n",
    "        for idx,var in enumerate(self.trainable_variables):\n",
    "            temp = tf.placeholder(tf.float32, name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(temp)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss_function, self.trainable_variables)\n",
    "        \n",
    "        optimizer = self.GDFunction(learning_rate=self.learning_rate)\n",
    "        self.updated_weights = optimizer.apply_gradients(zip(self.gradient_holders,self.trainable_variables))\n",
    "    \n",
    "    def set_up_gradient_holder(self, session):\n",
    "        self.grad_buffer=session.run(self.trainable_variables)\n",
    "        for index, gradient in enumerate(self.grad_buffer):\n",
    "            self.grad_buffer[index] = gradient * 0 # here we just want the sizes\n",
    "\n",
    "    def update_gradients(self, session, feed_dict):\n",
    "        tempgradients = session.run(self.gradients, feed_dict=feed_dict)\n",
    "        for index, gradient in enumerate(tempgradients):\n",
    "            self.grad_buffer[index] -= gradient\n",
    "\n",
    "    def update_batch_gradients(self, session):\n",
    "        feed_dict = dictionary = dict(zip(self.gradient_holders, self.grad_buffer))\n",
    "        _ = session.run(self.updated_weights, feed_dict=feed_dict)\n",
    "        for index, gradient in enumerate(self.grad_buffer):\n",
    "            self.grad_buffer[index] = gradient * 0\n",
    "        \n",
    "    def choose_action(self, session, s):\n",
    "        #Probabilistically pick an action given our network outputs.\n",
    "        action_probabilities = session.run(self.output_layer,feed_dict={self.state:[s]})\n",
    "        action = np.random.choice(action_probabilities[0],p=action_probabilities[0])\n",
    "        action = np.argmax([action_probabilities == action])\n",
    "        new_state, reward, done, info = self.environment.step(action)\n",
    "        self.environment.render()\n",
    "        return [s,action, new_state, reward], done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_gamma(list_of_rewards):\n",
    "    index = np.arange(len(list_of_rewards))\n",
    "    gammas = np.power(gamma, index) # Awjuliani misses this step\n",
    "    vectorized_mult = np.multiply(gammas, list_of_rewards)\n",
    "    return vectorized_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0\n",
      "34.64\n",
      "40.76\n",
      "39.74\n",
      "41.62\n",
      "56.6\n",
      "63.22\n",
      "79.72\n",
      "91.29\n",
      "95.76\n",
      "102.68\n",
      "112.93\n",
      "107.67\n",
      "124.08\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "repeats = 999\n",
    "max_games = 5000\n",
    "agent = rl_agent(\n",
    "    environment=env,\n",
    "    state_space_size=4,\n",
    "    action_space_size=2,\n",
    "    hidden_units=8,\n",
    "    learning_rate=0.01,\n",
    "    epsilon=1e-8, \n",
    "    gradient_descent_function=tf.train.AdamOptimizer)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    init = session.run(init)\n",
    "    i = 0\n",
    "    rewards = []\n",
    "    time_above_ground = []\n",
    "    agent.set_up_gradient_holder(session)\n",
    "    while i < max_games:\n",
    "        state = env.reset()\n",
    "        env.render()\n",
    "        reward_for_game = 0\n",
    "        history = [] # 4 items from past\n",
    "        for j in xrange(repeats):\n",
    "            history_element, done = agent.choose_action(session, state)\n",
    "            history.append(history_element)\n",
    "            state = history_element[-2]\n",
    "            reward_for_game += history_element[-1]\n",
    "            if done:\n",
    "                history = np.array(history)\n",
    "                history[:,3] = apply_gamma(history[:,3])\n",
    "                feed_dict = {agent.reward_tensor: history[:,3],\n",
    "                             agent.action_tensor: history[:,1],\n",
    "                             agent.state: np.vstack(history[:,0])}\n",
    "                agent.update_gradients(session, feed_dict)\n",
    "                if i % 5 == 0 and i != 0:\n",
    "                    agent.update_batch_gradients(session)\n",
    "                rewards.append(reward_for_game)\n",
    "                time_above_ground.append(j)\n",
    "                break\n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(rewards[-100:]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  206.,   302.,   305.,   201.,   180.,   196.,   210.,   243.,\n",
       "          233.,  2924.]),\n",
       " array([   9. ,   28.1,   47.2,   66.3,   85.4,  104.5,  123.6,  142.7,\n",
       "         161.8,  180.9,  200. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110b1b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
