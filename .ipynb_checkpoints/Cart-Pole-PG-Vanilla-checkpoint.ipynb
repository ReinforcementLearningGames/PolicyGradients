{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole - Policy Gradient Method\n",
    "## Policy Gradients Vanilla type\n",
    "* Borrowed heavily from structure of: [Vanilla Policy Gradient](https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb)\n",
    "* In order to determine positive and negative rewards we must associate it with some direction of the pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim  # Easy model building, not quite sure yet how to build nodes by hand\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 17:07:18,285] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(env.observation_space)  # 4x1 Box (vector-like)\n",
    "print(env.action_space)       # 2x1 Discrete\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rl_agent():\n",
    "    def __init__(self,  \n",
    "                 environment, \n",
    "                 state_space_size, \n",
    "                 action_space_size,         # For CartPole-v0\n",
    "                 hidden_units,\n",
    "                 learning_rate,             # Lambda or other for gradient descent\n",
    "                 epsilon,                   # Error - error for types of gradient descent or random choice\n",
    "                 gradient_descent_function):\n",
    "        self.environment = environment\n",
    "        self.hidden_units = hidden_units\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.GDFunction = gradient_descent_function\n",
    "        \n",
    "        # TF Vars\n",
    "        self.state = tf.placeholder(shape=[None,self.state_space_size], dtype=tf.float32)\n",
    "        \n",
    "        # ReLu is default\n",
    "        self.hidden_layer = slim.fully_connected(self.state,\n",
    "                                                 self.hidden_units,\n",
    "                                                 biases_initializer=None,\n",
    "                                                 activation_fn=tf.nn.relu)\n",
    "        # Output with softmax (only two possible choices, but faster to do softmax)\n",
    "        self.output_layer = slim.fully_connected(self.hidden_layer,\n",
    "                                                 self.action_space_size,\n",
    "                                                 biases_initializer=None,\n",
    "                                                 activation_fn=tf.nn.softmax)\n",
    "        self.current_action = tf.argmax(self.output_layer,1)\n",
    "        self.reward_tensor = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_tensor = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        # Range from 0 to the output dimension -- an index range [ 0, 1, 2...\n",
    "        output_range = tf.range(0, tf.shape(self.output_layer)[0])\n",
    "        # Basically this outputs the action \n",
    "        self.indexes = output_range * tf.shape(self.output_layer)[1] + self.action_tensor\n",
    "        # Formed an action tensor \n",
    "        self.output_tensor = tf.gather(tf.reshape(self.output_layer, [-1]), self.indexes)\n",
    "        # Basically a spread and gather according to the index yielding output\n",
    "        # which will be like y * y^ using the reduce mean here so we can reduce to 1x1\n",
    "        self.loss_function = -tf.reduce_mean(tf.log(self.output_tensor)*self.reward_tensor)\n",
    "        \n",
    "        self.trainable_variables = tf.trainable_variables()\n",
    "        \n",
    "        self.gradient_holders = []\n",
    "        # Each trainable variable is a partial derivative, but here they are just placeholders\n",
    "        for idx,var in enumerate(self.trainable_variables):\n",
    "            temp = tf.placeholder(tf.float32, name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(temp)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss_function, self.trainable_variables)\n",
    "        # Here we use GD, but others might use other optimizers\n",
    "        optimizer = self.GDFunction(learning_rate=self.learning_rate)\n",
    "        self.updated_weights = optimizer.apply_gradients(zip(self.gradient_holders,self.trainable_variables))\n",
    "    \n",
    "    def set_up_gradient_holder(self, session):\n",
    "        self.grad_buffer=session.run(self.trainable_variables)\n",
    "        for index, gradient in enumerate(self.grad_buffer):\n",
    "            self.grad_buffer[index] = gradient * 0 # here we just want the sizes\n",
    "\n",
    "    def update_gradients(self, session, feed_dict):\n",
    "        # Here the gradients are run and we keep the gradients in the buffer \n",
    "        # This allows us to update in the direction of the gradient\n",
    "        # For opposite of loss\n",
    "        tempgradients = session.run(self.gradients, feed_dict=feed_dict)\n",
    "        for index, gradient in enumerate(tempgradients):\n",
    "            self.grad_buffer[index] -= gradient\n",
    "\n",
    "    def update_batch_gradients(self, session):\n",
    "        # The gradients are reset for each episode but the model holds onto the weights\n",
    "        feed_dict = dictionary = dict(zip(self.gradient_holders, self.grad_buffer))\n",
    "        # We don't capture the results because the model does (I really which I understood how TF works more to handle this)\n",
    "        _ = session.run(self.updated_weights, feed_dict=feed_dict)\n",
    "        for index, gradient in enumerate(self.grad_buffer):\n",
    "            self.grad_buffer[index] = gradient * 0\n",
    "        \n",
    "    def choose_action(self, session, s):\n",
    "        # Probabilistically pick an action given our network outputs.\n",
    "        # We take the output of the NN as the probability distribution for the action\n",
    "        # Argmax is not actually necessary if we choose one in the random.choice function\n",
    "        action_probabilities = session.run(self.output_layer,feed_dict={self.state:[s]})\n",
    "        action = np.random.choice(action_probabilities[0],p=action_probabilities[0])\n",
    "        action = np.argmax([action_probabilities == action])\n",
    "        new_state, reward, done, info = self.environment.step(action)\n",
    "        self.environment.render()\n",
    "        return [s,action, new_state, reward], done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_gamma(list_of_rewards):\n",
    "    index = np.arange(len(list_of_rewards))\n",
    "    gammas = np.power(gamma, index) # Awjuliani misses this step\n",
    "    vectorized_mult = np.multiply(gammas, list_of_rewards)\n",
    "    return vectorized_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0\n",
      "30.17\n",
      "41.85\n",
      "69.19\n",
      "78.15\n",
      "103.23\n",
      "143.26\n",
      "166.01\n",
      "172.37\n",
      "178.78\n",
      "180.81\n",
      "187.61\n",
      "187.63\n",
      "192.16\n",
      "192.82\n",
      "193.96\n",
      "193.27\n",
      "195.09\n",
      "191.76\n",
      "189.46\n",
      "190.37\n",
      "186.56\n",
      "197.93\n",
      "195.41\n",
      "198.7\n",
      "198.6\n",
      "199.02\n",
      "199.72\n",
      "199.04\n",
      "200.0\n",
      "198.62\n",
      "198.67\n",
      "198.78\n",
      "198.96\n",
      "200.0\n",
      "198.88\n",
      "199.65\n",
      "197.25\n",
      "199.75\n",
      "198.12\n",
      "200.0\n",
      "199.13\n",
      "199.65\n",
      "198.71\n",
      "197.56\n",
      "198.91\n",
      "199.21\n",
      "197.2\n",
      "199.45\n",
      "199.26\n",
      "198.47\n",
      "195.81\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "repeats = 3000\n",
    "max_games = 10000\n",
    "agent = rl_agent(\n",
    "    environment=env,\n",
    "    state_space_size=4,\n",
    "    action_space_size=2,\n",
    "    hidden_units=8,\n",
    "    learning_rate=0.01,\n",
    "    epsilon=1e-8, \n",
    "    gradient_descent_function=tf.train.AdamOptimizer)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    init = session.run(init)\n",
    "    i = 0\n",
    "    rewards = []\n",
    "    time_above_ground = []\n",
    "    agent.set_up_gradient_holder(session)\n",
    "    while i < max_games:\n",
    "        state = env.reset()\n",
    "        env.render()\n",
    "        reward_for_game = 0\n",
    "        history = [] # 4 items from past\n",
    "        for j in xrange(repeats):\n",
    "            # Capture the results from each action here so we can feed it to TF to update\n",
    "            # the gradients at the completion of each episode (when done)\n",
    "            history_element, done = agent.choose_action(session, state)\n",
    "            history.append(history_element)\n",
    "            state = history_element[-2]\n",
    "            reward_for_game += history_element[-1]\n",
    "            if done:\n",
    "                # Convert to numpy array from list so we can take columns from it\n",
    "                # There are many ways to do this, Juliani likes this way\n",
    "                history = np.array(history)\n",
    "                history[:,3] = apply_gamma(history[:,3])\n",
    "                feed_dict = {agent.reward_tensor: history[:,3],\n",
    "                             agent.action_tensor: history[:,1],\n",
    "                             agent.state: np.vstack(history[:,0])}\n",
    "                agent.update_gradients(session, feed_dict)\n",
    "                if i % 5 == 0 and i != 0:\n",
    "                    agent.update_batch_gradients(session)\n",
    "                rewards.append(reward_for_game)\n",
    "                time_above_ground.append(j)\n",
    "                break\n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(rewards[-100:]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
