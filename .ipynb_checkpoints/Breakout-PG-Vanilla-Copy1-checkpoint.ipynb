{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout - Policy Gradient Method\n",
    "## Policy Gradients Vanilla type\n",
    "* Borrowed heavily from structure of: [Vanilla Policy Gradient](https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb)\n",
    "* In order to determine positive and negative rewards we must associate it with some direction of the pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, convolutional, pooling, Flatten, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as Bk\n",
    "from scipy.misc import imread, imshow\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-02 15:45:36,926] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "Discrete(6)\n",
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "print(env.observation_space)  # 210v x 160w x 3color Box (sparse tensor!!!)\n",
    "print(env.action_space)       # 6x1 Discrete\n",
    "s = env.reset()\n",
    "print(s.shape)\n",
    "gamma = 0.99\n",
    "try: \n",
    "    xrange(1)\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rl_agent():\n",
    "    def __init__(self,  \n",
    "                 environment, \n",
    "                 state_space_size, \n",
    "                 action_space_size,               \n",
    "                 hidden_units = 400,\n",
    "                 learning_rate = 1e-3,             # Lambda or other for gradient descent\n",
    "                 epsilon = 1e-8,                  # Error - error for types of gradient descent or random choice\n",
    "                 random_choice_threshold = -.72,\n",
    "                 optimizer = Adam,\n",
    "                 dropout_rate = 0.3,\n",
    "                 load_file_weights = None,\n",
    "                 verbose = 0,\n",
    "                 frames = 3\n",
    "                ):\n",
    "        self.environment = environment\n",
    "        self.hidden_units = hidden_units # hidden neurons\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.optimizer = optimizer\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.load_file_weights = load_file_weights\n",
    "        self.verbose = verbose\n",
    "        self.frames = frames\n",
    "        self.input_dim = 210*160\n",
    "        self.prev_processed_state = None\n",
    "        self.past_differences = []\n",
    "        self.render = False\n",
    "        self.random_choice_threshold = random_choice_threshold\n",
    "        \n",
    "    def define_model(self):\n",
    "        # Keras vars\n",
    "        self.model = Sequential()\n",
    "        self.model.add(convolutional.Conv2D(filters=32, kernel_size = (3,3), strides=(1,1), padding='same',\n",
    "            input_shape = (1, 210, 160), activation='relu'))\n",
    "        self.model.add(pooling.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "        self.model.add(convolutional.Conv2D(filters=64, kernel_size = (5,5),\n",
    "            strides=(1,1), padding='same', activation='relu'))\n",
    "        self.model.add(pooling.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(self.hidden_units, activation='relu'))   # Karpathy suggested N=200 in the hidden layer\n",
    "        self.model.add(Dropout(self.dropout_rate))      # But since we are doing dropout to avoid overfitting... increase\n",
    "        self.model.add(Dense(6, activation='softmax'))  # Actions are a 6x1 vector\n",
    "        optimizer = self.optimizer(lr=self.learning_rate)\n",
    "        self.model.compile(\n",
    "            optimizer = optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "             self.model.load_weights('saved_weights3.h5')\n",
    "        except:\n",
    "            print(\"Training model without old weights\")\n",
    "            \n",
    "        # State space size will be a 210x160x1 dim\n",
    "        self.state = Input(shape=self.state_space_size)\n",
    "        self.state1 = Input(shape=self.state_space_size)\n",
    "        #  self.current_action = tf.argmax(self.output_layer,1)\n",
    "        self.reward_tensor = Input(shape=(1,), dtype='float32')\n",
    "        self.action_tensor = Input(shape=(1,), dtype='int32')\n",
    "        \n",
    "        if self.verbose >= 1:\n",
    "            print(self.model.summary())\n",
    "        \n",
    "    def training(self, max_games = 100000):\n",
    "        mean_rewards = []\n",
    "        self.state = self.environment.reset()\n",
    "        chosen_vectors, rewards = [], []\n",
    "        running_reward = 0\n",
    "        reward_for_episode = 0\n",
    "        i = 0\n",
    "        while True:\n",
    "            history_element, done = self.choose_action()\n",
    "            chosen_vectors.append(history_element[4])\n",
    "            reward_for_episode += float(history_element[3])\n",
    "            rewards.append(history_element[3])\n",
    "            if done:\n",
    "                i += 1\n",
    "                all_episode_features = np.vstack(agent.past_differences)\n",
    "                all_episode_chosen_vectors = np.vstack(chosen_vectors)\n",
    "                all_episode_rewards = np.vstack(rewards)\n",
    "                discounted_episode_rewards = apply_gamma(all_episode_rewards)\n",
    "                # Standard normal feature scaling \n",
    "                mean_discounted_rewards = np.mean(discounted_episode_rewards)\n",
    "                discounted_episode_rewards -= mean_discounted_rewards\n",
    "                discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
    "                all_episode_chosen_vectors = np.multiply(discounted_episode_rewards.transpose(), all_episode_chosen_vectors.transpose()).transpose()\n",
    "                self.model.fit(all_episode_features.reshape([all_episode_features.shape[0], 1, 210,160]), all_episode_chosen_vectors, epochs=1, verbose=0, shuffle=True)\n",
    "                self.past_differences = []\n",
    "                self.prev_processed_state = None\n",
    "                chosen_vectors = []\n",
    "                rewards = []\n",
    "                reward_for_episode = 0\n",
    "                self.state = self.environment.reset()\n",
    "                if i%100 == 0:\n",
    "                    self.saveweights()\n",
    "                    mean_rewards.append(mean_discounted_rewards)\n",
    "                    print(\"MEAN DISCOUNTED REWARDS:\" + str(mean_discounted_rewards))\n",
    "                    \n",
    "    def savemodel(self):\n",
    "        with open('saved_model2.json') as modelfile:\n",
    "            modelfile.write(self.model.to_json())\n",
    "            \n",
    "    def saveweights(self):\n",
    "        self.model.save_weights('saved_weights3.h5', overwrite=True)\n",
    "        \n",
    "    def choose_action(self):\n",
    "        #Probabilistically pick an action given our network outputs.\n",
    "        s = self.preprocess(self.state)\n",
    "        #s = self.state\n",
    "        self.prev_processed_state = self.prev_processed_state if self.prev_processed_state is not None else np.zeros((210, 160), dtype='float32')\n",
    "        difference_processed = (s - self.prev_processed_state)\n",
    "        self.prev_processed_state = s\n",
    "        # difference_processed = difference_processed.flatten()\n",
    "        self.past_differences.append(difference_processed.flatten())\n",
    "        self.action_probabilities = self.model.predict(difference_processed.reshape([1,1,210,160])).flatten()\n",
    "        e = np.random.normal()\n",
    "        if (e < self.random_choice_threshold):\n",
    "            action = np.random.choice(self.environment.action_space.n, 1)[0]\n",
    "        else:\n",
    "            action = np.random.choice(self.environment.action_space.n, 1,p=self.action_probabilities/np.sum(self.action_probabilities))[0]\n",
    "        self.state, reward, done, info = self.environment.step(action)\n",
    "        if self.render: self.environment.render()\n",
    "        chosen_vector = np.zeros([self.environment.action_space.n])\n",
    "        chosen_vector[action] = 1\n",
    "        return [s,action, self.state, reward, chosen_vector], done\n",
    "    \n",
    "    def preprocess(self, state):\n",
    "        # take a 210x160x3 image and convert to 210x160x1 as 210*160 length vector flattened F = float32 in dim\n",
    "        # From https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html\n",
    "        r, g, b = state[:, :, 0], state[:, :, 1], state[:, :, 2]\n",
    "        F = r * 299.0/1000 + g * 587.0/1000 + b * 114.0/1000\n",
    "        return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gamma(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    r = r.flatten()\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model without old weights\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 1, 210, 32)        46112     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 105, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 105, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 53, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3392)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               1357200   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 2406      \n",
      "=================================================================\n",
      "Total params: 1,456,982\n",
      "Trainable params: 1,456,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:89: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:124: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN DISCOUNTED REWARDS:0.621538743207\n",
      "MEAN DISCOUNTED REWARDS:0.303492465365\n",
      "MEAN DISCOUNTED REWARDS:0.0\n",
      "MEAN DISCOUNTED REWARDS:0.0\n",
      "MEAN DISCOUNTED REWARDS:0.56799500607\n",
      "MEAN DISCOUNTED REWARDS:0.919003533687\n",
      "MEAN DISCOUNTED REWARDS:0.348507403916\n",
      "MEAN DISCOUNTED REWARDS:0.266367770714\n",
      "MEAN DISCOUNTED REWARDS:0.67675553202\n"
     ]
    }
   ],
   "source": [
    "agent = rl_agent(\n",
    "    environment=env,\n",
    "    state_space_size=(210,160,1),\n",
    "    action_space_size=6,\n",
    "    hidden_units=400,\n",
    "    epsilon=1e-8, \n",
    "    optimizer=Adam,\n",
    "    verbose=1)\n",
    "agent.define_model()\n",
    "agent.training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
