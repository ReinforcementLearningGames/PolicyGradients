{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout - Policy Gradient Method\n",
    "## Policy Gradients Vanilla type\n",
    "* Borrowed heavily from structure of: [Vanilla Policy Gradient](https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb)\n",
    "* In order to determine positive and negative rewards we must associate it with some direction of the pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, convolutional, pooling, Flatten, Dropout, Input, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as Bk\n",
    "from scipy.misc import imread, imshow\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-18 20:41:14,721] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "Discrete(6)\n",
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "print(env.observation_space)  # 210v x 160w x 3color Box (sparse tensor!!!)\n",
    "print(env.action_space)       # 6x1 Discrete\n",
    "s = env.reset()\n",
    "print(s.shape)\n",
    "gamma = 0.99\n",
    "try: \n",
    "    xrange(1)\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rl_agent():\n",
    "    def __init__(self,  \n",
    "                 environment, \n",
    "                 state_space_size, \n",
    "                 action_space_size,               \n",
    "                 hidden_units = 512,\n",
    "                 learning_rate = 1e-4,             # Lambda or other for gradient descent\n",
    "                 epsilon = 1e-8,                  # Error - error for types of gradient descent or random choice\n",
    "                 random_choice_threshold = -1.22,\n",
    "                 optimizer = Adam,\n",
    "                 dropout_rate = 0.6,\n",
    "                 load_file_weights = None,\n",
    "                 verbose = 0,\n",
    "                 frames = 3\n",
    "                ):\n",
    "        self.environment = environment\n",
    "        self.hidden_units = hidden_units # hidden neurons\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.optimizer = optimizer\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.load_file_weights = load_file_weights\n",
    "        self.verbose = verbose\n",
    "        self.frames = frames\n",
    "        self.input_dim = 6400\n",
    "        self.prev_processed_state = None\n",
    "        self.past_differences = []\n",
    "        self.render = False\n",
    "        self.random_choice_threshold = random_choice_threshold\n",
    "        \n",
    "    def define_model(self):\n",
    "        # Keras vars\n",
    "        self.model = Sequential()\n",
    "        # Default is channels last\n",
    "        self.model.add(convolutional.Conv2D(filters=64, kernel_size = (3,3), strides=(2,2), padding='valid', input_shape = (80,80, 1)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        print(self.model.layers[-1].input_shape)\n",
    "        print(self.model.layers[-1].output_shape)\n",
    "        self.model.add(convolutional.Conv2D(filters=32, kernel_size = (3,3), strides=(2, 2), padding='same'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(convolutional.Conv2D(filters=64, kernel_size = (3,3), strides=(2, 2), padding='valid'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(convolutional.Conv2D(filters=64, kernel_size = (3,3), strides=(1, 1), padding='valid'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        print(self.model.layers[-1].input_shape)\n",
    "        print(self.model.layers[-1].output_shape)\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(self.hidden_units))   # Karpathy suggested N=200 in the hidden layer\n",
    "        self.model.add(Dropout(self.dropout_rate))      # But since we are doing dropout to avoid overfitting... increase\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(self.hidden_units))   # Karpathy suggested N=200 in the hidden layer\n",
    "        self.model.add(Dropout(self.dropout_rate))      # But since we are doing dropout to avoid overfitting... increase\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        print(self.model.layers[-1].input_shape)\n",
    "        print(self.model.layers[-1].output_shape)\n",
    "        self.model.add(Dense(6, activation='softmax'))  # Actions are a 6x1 vector\n",
    "        optimizer = self.optimizer(lr=self.learning_rate, beta_1=0.9, beta_2=0.999, epsilon = self.epsilon, decay=0.0)\n",
    "        self.model.compile(\n",
    "            optimizer = optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "             self.model.load_weights('saved_weights5.h5')\n",
    "        except:\n",
    "            print(\"Training model without old weights\")\n",
    "            \n",
    "        # State space size will be a 210x160x1 dim\n",
    "        self.state = Input(shape=[80,80,1])\n",
    "        #  self.current_action = tf.argmax(self.output_layer,1)\n",
    "        self.reward_tensor = Input(shape=(1,), dtype='float32')\n",
    "        self.action_tensor = Input(shape=(1,), dtype='int32')\n",
    "        \n",
    "        if self.verbose >= 1:\n",
    "            print(self.model.summary())\n",
    "        \n",
    "    def training(self, max_games = 100000):\n",
    "        mean_rewards_list = []\n",
    "        self.state = self.environment.reset()\n",
    "        chosen_vectors, rewards, rewards_history = [], [], []\n",
    "        running_reward = 0\n",
    "        reward_for_episode = 0\n",
    "        i = 0\n",
    "        while True:\n",
    "            history_element, done = self.choose_action()\n",
    "            chosen_vectors.append(history_element[4]) # dlogps\n",
    "            reward_for_episode += float(history_element[3]) \n",
    "            rewards.append(history_element[3]) # drs\n",
    "            rewards_history.append(history_element[3]) \n",
    "            if done:\n",
    "                i += 1\n",
    "                all_episode_features = np.vstack(agent.past_differences) # epx\n",
    "                plt.imshow(agent.state)\n",
    "                all_episode_chosen_vectors = np.vstack(chosen_vectors) #epdlogp\n",
    "                all_episode_rewards = np.vstack(rewards) # epr\n",
    "                discounted_episode_rewards = apply_gamma(all_episode_rewards)\n",
    "                # Standard normal feature scaling \n",
    "                std_dev = np.std(discounted_episode_rewards)+self.epsilon\n",
    "                discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
    "                discounted_episode_rewards /= std_dev\n",
    "                all_episode_chosen_vectors *= discounted_episode_rewards\n",
    "                self.model.fit(all_episode_features.reshape([-1,80,80,1]), all_episode_chosen_vectors, epochs=1, verbose=0)\n",
    "                self.prev_processed_state = None\n",
    "                reward_for_episode = 0\n",
    "                self.state = self.environment.reset()\n",
    "                self.past_differences = []\n",
    "                chosen_vectors = []\n",
    "                rewards = []\n",
    "                if i % 50 == 0 and i != 0:\n",
    "                    print(len(rewards_history))\n",
    "                    mean_rewards = np.mean(rewards_history[-50:])\n",
    "                    self.saveweights()\n",
    "                    self.savemodel()\n",
    "                    mean_rewards_list.append(mean_rewards)\n",
    "                    print(\"MEAN REWARDS:\" + str(mean_rewards))\n",
    "                    \n",
    "    def savemodel(self):\n",
    "        with open('saved_model5.json', 'w') as modelfile:\n",
    "            modelfile.write(self.model.to_json())\n",
    "            \n",
    "    def saveweights(self):\n",
    "        self.model.save_weights('saved_weights5.h5', overwrite=True)\n",
    "        \n",
    "    def choose_action(self):\n",
    "        #Probabilistically pick an action given our network outputs.\n",
    "        s = self.preprocess(self.state)\n",
    "        self.state = s\n",
    "        self.prev_processed_state = self.prev_processed_state if self.prev_processed_state is not None else np.zeros((1, 80,80, 1), dtype='float32')\n",
    "        difference_processed = (s - self.prev_processed_state)\n",
    "        self.prev_processed_state = s\n",
    "        # difference_processed = difference_processed.flatten()\n",
    "        self.past_differences.append(difference_processed)\n",
    "        self.action_probabilities = self.model.predict(difference_processed.reshape([-1,80,80,1]), batch_size=1).flatten()\n",
    "        e = np.random.normal()\n",
    "        if (e < self.random_choice_threshold):\n",
    "            action = np.random.choice(self.environment.action_space.n, 1)[0]\n",
    "        else:\n",
    "            action = np.random.choice(self.environment.action_space.n, 1,p=self.action_probabilities/np.sum(self.action_probabilities))[0]\n",
    "        self.state, reward, done, info = self.environment.step(action)\n",
    "        if self.render: self.environment.render()\n",
    "        chosen_vector = np.zeros([self.environment.action_space.n])\n",
    "        chosen_vector[action] = 1\n",
    "        return [s,action, self.state, reward, chosen_vector], done\n",
    "    \n",
    "    def preprocess(self, state):\n",
    "        # take a 210x160x3 image and convert to 210x160x1 as 210*160 length vector flattened F = float32 in dim\n",
    "        # From https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html\n",
    "        #r, g, b = state[:, :, 0], state[:, :, 1], state[:, :, 2]\n",
    "        #F = r * 299.0/1000 + g * 587.0/1000 + b * 114.0/1000\n",
    "        # Now using carpathy 0/1 conversion of a majority of the surface area of the screen down to 80x80\n",
    "        F = state\n",
    "        F = F[33:193] # crop vertical rows to 160\n",
    "        F = F[::2,::2,0] # downsample by factor of 2 to 90x80 just keeps the first index of the 3rd dim\n",
    "        F[F != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "        F[F != 1] = 0\n",
    "        return F.reshape([1,80,80,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_gamma(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    r = r.flatten()\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 39, 39, 64)\n",
      "(None, 39, 39, 64)\n",
      "(None, 7, 7, 64)\n",
      "(None, 7, 7, 64)\n",
      "(None, 1024)\n",
      "(None, 1024)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 39, 39, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 39, 39, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 39, 39, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 20, 20, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 20, 20, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 9, 9, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 4,351,654\n",
      "Trainable params: 4,347,110\n",
      "Non-trainable params: 4,544\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "agent = rl_agent(\n",
    "    environment=env,\n",
    "    state_space_size=(6400,1),\n",
    "    action_space_size=6,\n",
    "    hidden_units=1024,\n",
    "    epsilon=1e-8, \n",
    "    optimizer=Adam,\n",
    "    verbose=1)\n",
    "agent.define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11597\n",
      "MEAN REWARDS:0.02\n",
      "23278\n",
      "MEAN REWARDS:0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-97823af8dd5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-3836f524cf4f>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(self, max_games)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mhistory_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[0mchosen_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_element\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# dlogps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mreward_for_episode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_element\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-3836f524cf4f>\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_probabilities\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_probabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mchosen_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \"\"\"\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \"\"\"\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carl-PC\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\atari_py-0.0.18-py3.5.egg\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDpFZkkNkluQQmSU5RGZJ/wdEvFyzWAyZuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27c636746d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27c65cd6f60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZJbkEJklOURmSQ6RWZJDZJb0f4maYI/Zjq5YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27c69d9ecc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s,r,d,i = env.step(0)\n",
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200  72  72]\n"
     ]
    }
   ],
   "source": [
    "print(s[190][115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
